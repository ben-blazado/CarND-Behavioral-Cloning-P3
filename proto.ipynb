{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "departmental-metallic",
   "metadata": {},
   "source": [
    "Readme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-narrative",
   "metadata": {},
   "source": [
    "# Behavioral Cloning\n",
    "This is a Udacity Self-Driving Car NanoDegree project submission that uses deep learning to clone driving behavior.\n",
    "\n",
    "![](./wup_assets/mtn-turning.png)\n",
    "\n",
    "## Installation\n",
    "Clone or fork this repository. For dependencies, see `env-bcl-gpu.yml` for packages. Also requires\n",
    "[Udacity's Self-Driving Car Simulator](https://github.com/udacity/self-driving-car-sim).\n",
    "\n",
    "## Usage\n",
    "Intended user is the Udacity evaluator for this project. To view a video recording of the vehicle autonomously driving around a simulated mountainous track from the perspective of a camera mounted at the center of the vehicle, play the following mp4:\n",
    "\n",
    "`video.mp4`\n",
    "\n",
    "To directly observe the vehicle autonomously driving from a third person view, do the following:\n",
    "\n",
    "1. Start the autonomous driver: `python drive.py model.h5`\n",
    "2. Start Udacity's Self-Driving Car Simulator (SDCS), and click the play button.\n",
    "3. Select track to drive on in the SDCS.\n",
    "4. Select Autonomous Mode in the SDCS. This connects the simulator to `drive.py` which drives the car around the selected track based on the machine learning model captured in `model.h5`.\n",
    "\n",
    "## Files\n",
    "### Project Files\n",
    "- `model.py`: Python script used to create and train the model.\n",
    "- `drive.py`: Python script used to autonomously drive the car.\n",
    "- `model.h5` : The saved model.\n",
    "- `writeup_report.md`: writeup of project for Udacity evaluator.\n",
    "- `video.mp4`: video recording of vehicle driving around track 2 in the opposite direction.\n",
    "\n",
    "### Other files \n",
    "- Additional videos from 3rd person perspective:\n",
    "  - `auto_easy_3p.mp4`: driving around track 1.\n",
    "  - `auto_easy_rev_3p.mp4`: driving in opposite direction around track 1.\n",
    "  - `auto_hard_3p.mp4`: driving around track 2.\n",
    "  - `auto_hard_rev_3p.mp4`: driving in opposite direction around track 2.  \n",
    "- Additional videos from driver perspective:\n",
    "  - `auto_easy.mp4`: driving around track 1.\n",
    "  - `auto_easy_rev.mp4`: driving in opposite direction around track 1.\n",
    "  - `auto_hard.mp4`: driving around track 2.\n",
    "- `env-bcl-gpu.yml`: YAML file for installing other packages in a Conda environment.\n",
    "- `proto.ipynb`: Jupyter Notebook for prototyping python and markdown code.\n",
    "- `training_log.csv`: CSV file showing training history of `model.h5`.\n",
    "- `video.py`: Udacity included script for generating video of vehicle driving.\n",
    "- `data_easy_route`: contains training data consisting of driving log and images of recording of vehicle driving around track_1\n",
    "- `data_hard_route`: contains training data consisting of driving log and images of recording of vehicle driving around track_2\n",
    "\n",
    "\n",
    "\n",
    "## Training Strategy/Approach\n",
    "\n",
    "### Training Data\n",
    "\n",
    "- Use Sample Data Recorded Screen Captures of views from center, left and right camera\n",
    "- Example Images\n",
    "- The csv file showing steering angle\n",
    "\n",
    "- To better generalize, we record track 2\n",
    "\n",
    "- Generators\n",
    "- To augment we add shifted left or right from extreme left or right camera and appropriate\n",
    "- \n",
    "\n",
    "## Network Structure/Model Architecture\n",
    "\n",
    "- We use the nvidia model\n",
    "- Added dropout and used Rectified Linear Units for activations\n",
    "- \n",
    "\n",
    "![](./wup_assets/history.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-project",
   "metadata": {},
   "source": [
    "writeup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-catalyst",
   "metadata": {},
   "source": [
    "12-MAR-2021\n",
    "# Behavioral Cloning\n",
    "This is a Udacity Self-Driving Car NanoDegree project submission that uses deep learning to clone driving behavior.\n",
    "\n",
    "![](./wup_assets/2021_03_11_00_59_53_234.jpg)\n",
    "\n",
    "## Table of Contents\n",
    "- [**Required Files**](#required-files)\n",
    "- [**Dataset Exploration**](#dataset-exploration)\n",
    "  - [Data Summary](#data-summary)\n",
    "  - [Data Visualization](#data-visualization)\n",
    "  - [Sign Type Proportionality](#sign-type-proportionality)\n",
    "- [**Design and Test a Model Architecture**](#design-and-test-a-model-architecture)\n",
    "  - [Preprocessing](#preprocessing)\n",
    "    - [Generating Fake Images](#generating-fake-images)\n",
    "    - [Larger and Balanced Training Set](#larger-and-balanced-training-set)\n",
    "    - [Brightening Dark Images](#brightening-dark-images)\n",
    "    - [Normalizing Image Data](#normalizing-image-data)\n",
    "    - [Examples of Processed Image](#examples-of-processed-image)\n",
    "  - [Model Architecture](#model-architecture)\n",
    "  - [Model Training](#model-training)\n",
    "  - [Solution Approach](#solution-approach)\n",
    "  - [notLenet Performance](#notLenet-performance)\n",
    "    - [Validation Accuracy](#validation-accuracy)\n",
    "    - [Accuracy Across All Datasets](#accuracy-across-all-datasets)\n",
    "    - [By-Class Perfomance with Test Dataset](#by-class-perfomance-with-test-dataset)\n",
    "      - [Hardest Signs](#hardest-signs)\n",
    "      - [Easiest Signs](#easiest-signs)\n",
    "- [**Test a Model on New Images**](#test-a-model-on-new-images)\n",
    "  - [Acquiring New Images](#acquiring-new-images)\n",
    "  - [Performance on New Images](#performance-on-new-images)\n",
    "  - [Model Certainty and Softmax Probabilities](#model-certainty-and-softmax-probabilities)\n",
    "  - [A Closer Look at Bicycles Crossing Sign Classification](#a-closer-look-at-bicycles-crossing-sign-classification)\n",
    "- [**Visualize Layers of the Neural Network**](#visualize-layers-of-the-neural-network)\n",
    "  - [Layer 0 Visualization](#layer-0-visualization)\n",
    "  - [Layer 1 Visualization](#layer-1-visualization)\n",
    "- [**Build a Multiscale CNN**](#build-a-multiscale-cnn)\n",
    "  - [Concatenate Layer](#concatenate-layer)\n",
    "  - [MutiScale Archictecture](#mutiScale-archictecture)\n",
    "  - [Training](#training)\n",
    "  - [Most Difficult Signs](#most-difficult-signs)\n",
    "  - [Most Easy Signs](#most-easy-signs)\n",
    "  - [Accuracy with Wikipedia Signs](#accuracy-with-wikipedia-signs)\n",
    "  - [Visualize Softmax Probabilties](#visualize-softmax-probabilties)\n",
    "\n",
    "## Required Files\n",
    "- `model.py`: Python script used to create and train the model.\n",
    "- `drive.py`: Python script used to autonomously drive the car. This script has the following modifications:\n",
    "  - Desired speed was increased to 25.0 from the original 9.0.\n",
    "  - Vehicle will slow down aggressively when initiating a moderate turn, then returns to desired speed.\n",
    "- `model.h5` : The saved model.\n",
    "- `writeup_report.md`: writeup of project for Udacity evaluator.\n",
    "- `video.mp4`: video recording of vehicle driving around track 2 in the opposite direction from the driver's perspective. Track 2 was chosen for the recording because there were more exciting turns. The third person perspective of this video can be viewed in `auto_hard_rev_3p.mp4`.\n",
    "\n",
    "## Quality of Code\n",
    "\n",
    "### Code Functionality\n",
    "\n",
    "The model was trained with data from recording 2 laps of driving on track 1 and track 2 (mountainous road). The model was saved to `model.h5` and can be observed to successuly operate the simulation in track 1 and track2 using the following:\n",
    "\n",
    "`python drive.py model.h5`\n",
    "\n",
    "### Code Usability \n",
    "\n",
    "The script to create the model is `model.py`. It uses a data generator class, `DrivingLogSequence`, that inherits from `kera.utils.Sequence`. This class accesses data from two folders, `data_easy_route` and `data_hard_route`, to generate images and steering angle data for training rather than storing the entire folders of images and steerings angles into memory.\n",
    "\n",
    "The main function in `model.py` that creates the neural network is `create_model`. . The function `train_model` then trains the model using `DrivingLogSequence` data generators and saves the model to the file `model.h5`\n",
    "\n",
    "### Code Readability\n",
    "\n",
    "The code follows [PEP-8](https://www.python.org/dev/peps/pep-0008/) Style Guide as much as possible and [PEP-257](https://www.python.org/dev/peps/pep-0257/) Docstring Conventions. For instance:\n",
    "\n",
    "```python\n",
    "    # ...\n",
    "    def samples(self, camera, img, steering):\n",
    "        '''\n",
    "        Creates a list of image and steering data for \n",
    "        adding to a batch of samples. Used in __getitem__().\n",
    "        \n",
    "        Params\n",
    "            camera: name of camera; 'center', 'left', or 'right'.\n",
    "            img: rgb image of camera\n",
    "            steering: steering ANGLE (float) applied\n",
    "        '''\n",
    "    # ...\n",
    "```\n",
    "\n",
    "\n",
    "[\"Snake case\"](https://en.wikipedia.org/wiki/Snake_case) is predominatly used except for class names which use [\"Camel Case\"](https://en.wikipedia.org/wiki/Camel_case). For instance:\n",
    "\n",
    "```python\n",
    "# ...\n",
    "class DrivingLogSequence(Sequence):\n",
    "    '''\n",
    "    Generates driving log data for training. \n",
    "    Inherits from keras.utils.Sequence.\n",
    "    Used in call to Model.fit_generator.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def __init__ (self, driving_log, batch_size=16):\n",
    "        '''\n",
    "        Class initializer.\n",
    "        \n",
    "        Params:\n",
    "            driving_log: a list of dicts containing the following keys:\n",
    "                'center'   : filename to image from center camera\n",
    "                'left'     : filename to image from left camera\n",
    "                'right'    : filename to image from right camera\n",
    "                'steering' : steering ANGLE applied\n",
    "                \n",
    "            batch_size: number of samples to get from driving log\n",
    "        '''\n",
    "        \n",
    "        self.driving_log = driving_log\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        return\n",
    "# ...\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Model Architecture and Training Strategy\n",
    "\n",
    "### Model Architecure Employed\n",
    "\n",
    "The model first normalizes the image, then crops out about the upper and lower third the image to retain mostly an image of the road. The rest of the image uses convolutions, kernels, and dense layers similar to model in _\"End to end Larning for Self-Driving Cars\" by Bojarski et. al., 25APR2016_. \n",
    "\n",
    "![](./wup_assets/cnn_architecture.png)\n",
    "\n",
    "### Reducing Overfitting\n",
    "\n",
    "Rectified Linear Units serve as the activation functions for each layer and then followed by dropout to reduce overfitting. Adding the non-linearities and dropout helped seemed to have the following effect:\n",
    "\n",
    "- Decrease the training loss from a mean squred error of around 0.06 to 0.02. \n",
    "- The vehicle was able to drive in the opposite direction around each track using training data of driving around the default direction.\n",
    "\n",
    "Below is the portion of code from `create_model()` showing the use of dropout after almost every layer except the last. Empirically, it seemed that using a low dropout rate of 0.1 or 0.2 after the convolutions helped in reducing overfitting than using a dropout value of 0.5.\n",
    "\n",
    "```python\n",
    "# ...\n",
    "model = Sequential()\n",
    "model.add(Lambda(normalize, input_shape=(160,320,3)))\n",
    "model.add(Cropping2D(cropping=[(50, 20), (0, 0)]))\n",
    "\n",
    "model.add(Conv2D(filters=24, kernel_size=5, strides=2, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(filters=36, kernel_size=5, strides=2, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(filters=48, kernel_size=5, strides=2, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=3, strides=1, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=3, strides=1, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(10, activation='relu'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "#...\n",
    "```\n",
    "\n",
    "### Parameter Tuning\n",
    "\n",
    "The model uses an adam optimizer was used so a learning rate parameter was not chosen. This was sufficient in training the vechicle to succeffuly drive on both tracks succesffully.\n",
    "\n",
    "### Training Data Chosen\n",
    "\n",
    "The training data chosen was a set of images of driving two laps around tracks 1 and 2 with their associated steering angles . The training data is contained in the following:\n",
    "\n",
    "- `data_easy_route`: folder data from driving on track 1\n",
    "  - `driving_log.csv`: comma separated value of image filenames and associated steering angles\n",
    "  - `IMG`: folder containing images from center, left and right cameras of vehicles\n",
    "- `data_hard_route`: folder containing images and driving log from track 2\n",
    "  - `driving_log.csv`: comma separated value of image filenames and associated steering angles\n",
    "  - `IMG`: folder containing images from center, left and right cameras of vehicles\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Data Summary\n",
    "\n",
    "Loading and examining the data yielded the following characteristics:\n",
    "- Training Samples: 34,799\n",
    "- Testing Samples: 12,630\n",
    "- Image Shape: 32x32x3\n",
    "- Number of types of Signs: 43\n",
    "\n",
    "\n",
    "### Data Visualization\n",
    "\n",
    "Below are some images from each of the 43 types of traffic signs (from the Training Samples). Many images are very dark and will need brightness increased. Color, shape, and orientation matters for interpretation.\n",
    "\n",
    "![](wup_assets/TrafficSignsByType.png)\n",
    "\n",
    "### Sign Type Proportionality\n",
    "\n",
    "There is a large imbalance in the prorportion of each type of sign found in each dataset (below). \n",
    "\n",
    "For example, the proportion of \"Speed limit (20km/h)\" signs present in the dataset is much less than the proportion of \"Speed limit (50km/h)\" signs. \n",
    "\n",
    "Additional signs will be generated to fix this disproportion so the model has a better chance to be equally exposed to all types of signs during training and hopefully improve validation accuracy.\n",
    "\n",
    "![](wup_assets/PctByType.png)\n",
    "\n",
    "\n",
    "## Design and Test a Model Architecture\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "Fake images will be generated for the training dataset to address the imbalance in the number of signs for each type of sign. Images will then be brightened then normalized prior to submission for training. \n",
    "\n",
    "#### Generating Fake Images\n",
    "\n",
    "Similar to Sermanet and Lecun in _\"Traffic Sign Recognition with Multi-Scale Convolutional Networks\"_, shifting, rotation, and scaling (zooming in) is randomly performed on traffic signs generated to augment the dataset. These manipulations will expose the model to different variations of the generated image during training.\n",
    "\n",
    "```python\n",
    "def shift(image, d=None):\n",
    "    '''\n",
    "    Shifts image left, right, up or down from 1 to 6 pixels. \n",
    "    Used in generating additional training samples.\n",
    "    '''\n",
    "    if d is None:\n",
    "        d = random.randint(1, 6)\n",
    "    direction = random.choice([1, 2, 3, 4])\n",
    "    if direction == 1:\n",
    "        image[:-d] = image[d:]\n",
    "    elif direction == 2:\n",
    "        image[d:] = image[:-d]\n",
    "    elif direction == 3:\n",
    "        image[:,d:] = image[:,:-d]\n",
    "    else:\n",
    "        image[:,:-d] = image[:,d:]\n",
    "        \n",
    "    return image\n",
    "\n",
    "def crop(image, size=32):\n",
    "    '''\n",
    "    Crops image to 32x32. Used after image is zoomed in.\n",
    "    '''\n",
    "    sizes = np.array(image.shape[:2]) - 32\n",
    "    lower =  sizes // 2\n",
    "    upper = image.shape[:2] - (lower + (sizes % 2))\n",
    "    img = image[lower[0]:upper[0], lower[1]:upper[1]]\n",
    "    return img\n",
    "\n",
    "def zoom(image, scale=None):\n",
    "    '''\n",
    "    Zooms in on an image from 1.0x to 1.6x. Uses crop to ensure img is 32x32\n",
    "    Used in generating additional training samples.\n",
    "    '''\n",
    "    if scale is None:\n",
    "        scale = random.uniform(1.0, 1.6)\n",
    "    img = sk.transform.rescale(image, scale, multichannel=True, preserve_range=True).astype(np.uint8)\n",
    "    return crop(img)\n",
    "\n",
    "def rotate (image, deg=None):\n",
    "    '''\n",
    "    Rotates image from -15 to 15 degrees.\n",
    "    Used in generating additional training samples.\n",
    "    '''\n",
    "    if deg is None:\n",
    "        deg = random.uniform(-15, 15)\n",
    "    return sk.transform.rotate(image, deg, preserve_range=True).astype(np.uint8)\n",
    "```\n",
    "\n",
    "#### Larger and Balanced Training Set\n",
    "\n",
    "After adding the fake images, the training set grew from 34,799 to 259,290 samples. This increase in the amount of training data helped push the model to high validation accuracies during experimentation. The fake images were generated in such a way as to achieve a balance in proportion across each type of traffic sign.\n",
    "\n",
    "![](./wup_assets/EqualPortionsTraining.png)\n",
    "\n",
    "#### Brightening Dark Images\n",
    "\n",
    "`equalizeHist()` from `opencv` was used to brighten dark images. The image is first converted to the HSV colorspace. If the average value of the V-component, `mean_v`, is less than the default `v_thresh` of 128, `equalizeHist()` is applied:\n",
    "\n",
    "```python\n",
    "def equalizeHist(orgimg, v_thresh=128):\n",
    "    '''\n",
    "    Brightens dark images.\n",
    "    \n",
    "    Params:\n",
    "    - orgimg: original image (RGB)\n",
    "    - v_thresh: max integer of the average value of the image for brightening to occur\n",
    "    '''\n",
    "    hsv = cv2.cvtColor(orgimg, cv2.COLOR_RGB2HSV)\n",
    "    mean_v = np.mean(hsv[:,:,2])\n",
    "    if mean_v < v_thresh:\n",
    "        equ = cv2.equalizeHist(hsv[:,:,2])\n",
    "        hsv[:,:,2] = equ\n",
    "        img = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
    "    else:\n",
    "        img = orgimg\n",
    "    return img\n",
    "```\n",
    "\n",
    "#### Normalizing Image Data\n",
    "\n",
    "A guiding principle in developing the model architecture is for the inputs to have a mean of 0.0 (zero) and have equal variance (see video [Normalized Inputs and Initialization](https://youtu.be/WaHQ9-UXIIg?t=22)). This makes it easier for the Tensorflow optimizer to discover appropriate parameters (weights and biases) during training. The function `RGB_to_norm()` is applied to normalize the images:\n",
    "\n",
    "```python\n",
    "def RGB_to_norm(img):\n",
    "    return (np.float32(img) - 128)/128\n",
    "```\n",
    "\n",
    "#### Examples of Processed Image\n",
    "\n",
    "Below shows the effects of applying brightening, rotation, zoom, and shifting:\n",
    "\n",
    "![](./wup_assets/BrightenFake.png)\n",
    "\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "The model architecture generally follows the LeNet architecture outlined in _Lesson 13: Convolutional Neural Networks, Item 36. Lab: LeNet in Tensorflow_.\n",
    "\n",
    "A convolution with a 1x1 filter was used in the very first layer to produce output shape of 32x32x1. This forces the model to compress the 3 RGB channels of the Input layer into 1 channel, thus achieving a grayscaling effect, but allowing the model to determine the significance of each channel value. The improved validation accuracy was quite astonishing as compared to training the model without this 1x1 convolution in the first layer. \n",
    "\n",
    "The other convolutional layers have the same first and second dimensions as LeNet, but are deeper (24 and 64 respectively). The fully connected layers are wider with dropout added in the later stages. The  deepening of the convolutions, widening of the connections, and addition of dropout seemed to improve validation accuracy during experimentation.\n",
    "\n",
    "A custom set of high level classes (`Layers`, `Conv2D`, `Pooling`, `Flatten`, `Connected`, `Dropout`, `Model`, `Sequential`) which wrap Tensorflow are used to help code the model, named `notLenet`, in a Keras-like fashion.\n",
    "\n",
    "```python\n",
    "notLenet = Sequential(\"notLeNet\", input_shape=image_shape, n_classes=n_classes)\n",
    "\n",
    "notLenet.addLayer (Conv2D   ([32, 32, 1]))\n",
    "notLenet.addLayer (Conv2D   ([28, 28, 24]))\n",
    "notLenet.addLayer (Pooling  ([14, 14]))\n",
    "notLenet.addLayer (Conv2D   ([10, 10, 64]))\n",
    "notLenet.addLayer (Pooling  ([5, 5]))\n",
    "notLenet.addLayer (Flatten  ())\n",
    "notLenet.addLayer (Connected([240]))\n",
    "notLenet.addLayer (Dropout  ())    \n",
    "notLenet.addLayer (Connected([168]))\n",
    "notLenet.addLayer (Dropout  ())\n",
    "\n",
    "notLenet.assemble()\n",
    "```\n",
    "\n",
    "The model summary confirms the output shapes of each layer of the model:\n",
    "```\n",
    "Model-2944 |      Summary for notLeNet:\n",
    "Model-2944 | ----------------------------------\n",
    "Model-2944 |      Input     :(?, 32, 32, 3)\n",
    "Model-2944 | ----------------------------------\n",
    "Model-2944 | 0  : Conv2D    :(?, 32, 32, 1)\n",
    "Model-2944 | 1  : Conv2D    :(?, 28, 28, 24)\n",
    "Model-2944 | 2  : Pooling   :(?, 14, 14, 24)\n",
    "Model-2944 | 3  : Conv2D    :(?, 10, 10, 64)\n",
    "Model-2944 | 4  : Pooling   :(?, 5, 5, 64)\n",
    "Model-2944 | 5  : Flatten   :(?, 1600)\n",
    "Model-2944 | 6  : Connected :(?, 240)\n",
    "Model-2944 | 7  : Dropout   :(?, 240)\n",
    "Model-2944 | 8  : Connected :(?, 168)\n",
    "Model-2944 | 9  : Dropout   :(?, 168)\n",
    "Model-2944 | ----------------------------------\n",
    "Model-2944 |      Logits    :(?, 43)\n",
    "Model-2944 | ----------------------------------\n",
    "```\n",
    "\n",
    "### Model Training\n",
    "\n",
    "In `Model.connectLogits(),` the output logits and one-hot encoded labels were fed to Tensorflow's `tf.nn.softmax_cross_entropy_with_logits` to calculate the loss for each training batch. \n",
    "\n",
    "In `Model.train()`, the mean of this loss was fed to `tf.train.AdamOptimizer` which created the `minimizer` operation for use in `Session.run()`. \n",
    "\n",
    "\n",
    "```python\n",
    "def connectLogits(self, prev_layer):\n",
    "\n",
    "    self.logits = connected(prev_layer.tensor, [self.n_classes], activation=None)\n",
    "\n",
    "    oh_labels      = tf.one_hot(self.y, self.n_classes)\n",
    "    losses         = tf.nn.softmax_cross_entropy_with_logits(labels=oh_labels, logits=self.logits)\n",
    "    self.mean_loss = tf.reduce_mean(losses)\n",
    "    #...etc...\n",
    "    return\n",
    "\n",
    "def train(self, training_data, validation_data, epochs_done, batch_size, lr=0.001, \n",
    "          acc_save=0.93, acc_done=0.982, keep_prob=1.0, ):\n",
    "\n",
    "    #...etc...\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    minimizer = optimizer.minimize(self.mean_loss)\n",
    "    #...etc...\n",
    "    with tf.Session() as sess:\n",
    "        #...etc...\n",
    "        for offset in range(0, num_examples, batch_size):\n",
    "            #...etc...\n",
    "            sess.run(minimizer, feed_dict=feed_dict)\n",
    "```\n",
    "\n",
    "A batch size of `batch_size=128` was chosen through trial and error. Training terminates once `epochs_done=64` epochs have passed since the last highest validation accuracy was reached. If validation accuracy reaches `acc_done=0.997` (which it doesn't), training will also terminate. The model is saved every time a new high accuracy is achieved. Keep probability for the dropout layers is `keep_prob=0.5`. `keep_prob` is, however, set to 1.0 when calculating accuracy, precision, and recall in the `Model.measure()` method.\n",
    "\n",
    "```python\n",
    "notLenet.train(trainingSigns.data(), validSigns.data(), batch_size=128, \n",
    "        epochs_done=64, acc_done=0.997, keep_prob=0.5)\n",
    "```\n",
    "\n",
    "### Solution Approach\n",
    "\n",
    "The `Model.measure()` method is used to calculate validation accuracy after each epoch of training using Tensorflow's `tf.metrics.accuracy`. Note the accuracy is rounded to the third decimal:\n",
    "\n",
    " ```python\n",
    "acc = round (self.measure([tf.metrics.accuracy], validation_data)[0], 3)\n",
    "```\n",
    "\n",
    "`Model.measure()` uses a list of Tensorflow metrics, `tf_metrics` parameter, to calculate the desired metrics. In the case of calculating accuracy during training:\n",
    "\n",
    "```python\n",
    "tf_metrics = [tf.metrics.accuracy]\n",
    "```\n",
    "\n",
    "The `labels` and `predictions` for the accuracy operation simply uses the `tf.argmax()` of the one-hot encoded labels and logits respectively:\n",
    "\n",
    "```python\n",
    "labels      = tf.argmax(self.oh_labels, 1)\n",
    "predictions = tf.argmax(self.logits, 1)\n",
    "```\n",
    "\n",
    "The respective Tensorflow metrics operations (in this case `tf_metrics=[tf.metrics.accuracy]` are then created with:\n",
    "\n",
    "```python\n",
    "for i, tf_metric in enumerate(tf_metrics):\n",
    "    # setup tensorflow metrics precision tensor\n",
    "    name = 'metric_' + str(i)\n",
    "    op_metric, op_upd_metric = tf_metric(labels, predictions, name=name)\n",
    "```\n",
    "\n",
    "The update metric operation, `op_upd_metric`, is then used successively for each batch of validation data (`keep_prob` is set to 1.0 as mentioned earlier):\n",
    "\n",
    "```python\n",
    "for offset in range(0, n_samples, batch_size):\n",
    "\n",
    "    batch_x   = X_data[offset:offset+batch_size] \n",
    "    batch_y   = y_data[offset:offset+batch_size]\n",
    "    feed_dict = {self.x        : batch_x, \n",
    "                 self.y        : batch_y, \n",
    "                 self.keep_prob: 1.0}\n",
    "    \n",
    "    for op_upd_metric in op_upd_metrics:\n",
    "        sess.run(op_upd_metric, feed_dict=feed_dict)\n",
    "```\n",
    "\n",
    "The score of the metric is obtained by running `op_metric` once all batches are processed:\n",
    "\n",
    "```python\n",
    "scores = [sess.run(op_metric) for op_metric in op_metrics]\n",
    "```\n",
    "\n",
    "### notLenet Performance\n",
    "\n",
    "\n",
    "#### Validation Accuracy\n",
    "\n",
    "Training will continue as long as `epochs_done=64` epochs have not elapsed since the last highest accuracy score. notLenet's highest validation accuracy was __98.9%__ at epoch 49. The entire validation accuracy history is shown below:\n",
    "\n",
    "![](./wup_assets/notLenetValAcc.png)\n",
    "\n",
    "\n",
    "#### Accuracy Across All Datasets\n",
    "\n",
    "Accuracy across all datasets is plotted below:\n",
    "\n",
    "![](./wup_assets/notLenetAllAcc.png)\n",
    "\n",
    "#### By-Class Perfomance with Test Dataset\n",
    "\n",
    "By-class metrics can also be computed if a `classId` is passed to the `Model.measure()` method.  Instead of predicting the `classId` number of an input image, the model is made to predict if input image is or is not the traffic sign associated with `classId`. The labels and predictions are thus set like below:\n",
    "\n",
    "```python\n",
    "tf_classId  = tf.constant(classId, tf.int64)\n",
    "labels      = tf.equal(tf.argmax(self.oh_labels, 1), tf_classId)\n",
    "predictions = tf.equal(tf.argmax(self.logits, 1), tf_classId)\n",
    "```\n",
    "\n",
    "Additional performance metrics on the Testing data is calculated (by type of traffic sign) with the `Model.metrics_by_class()` method:\n",
    "\n",
    "```python\n",
    "accuracy, precision, recall = notLenet.metrics_by_class(testSigns.data())\n",
    "```\n",
    "\n",
    "##### Hardest Signs\n",
    "\n",
    "Averaging each of the `accuracy`, `precision`, and `recall` scores, the most difficult signs for notLenet to recognize can then be plotted.\n",
    "\n",
    "![](./wup_assets/notLenetHardest.png)\n",
    "\n",
    "##### Easiest Signs\n",
    "\n",
    "The most easy signs for notLenet to recognize are plotted as well:\n",
    "\n",
    "![](./wup_assets/notLenetEasiest.png)\n",
    "\n",
    "## Test a Model on New Images\n",
    "\n",
    "### Acquiring New Images\n",
    "\n",
    "Wikipedia images of German signs were used due to ease of access. Unlike the dataset where the traffic signs appear to be photographs, the aquired signs are graphics. \n",
    "\n",
    "The symbols, colors, and shapes appear clearly so the model should be able to classify them easily. \n",
    "\n",
    "However, the model has never seen graphics versions of the signs. The signs also had to be downsized to a lower resolution of 32x32 for the model to use.\n",
    "\n",
    "![](./wup_assets/WikipediaTrafficSigns.png)\n",
    "\n",
    "### Performance on New Images\n",
    "\n",
    "notLenet accuracy with the Wikipedia signs is caculated and, as expected, was able to classify all (100%) the acquired Wikipedia traffic signs correctly.  \n",
    "\n",
    "```python\n",
    "notLenet_acc = notLenet.metrics(wiki_traffic_signs.data(), metrics=[tf.metrics.accuracy])[0]\n",
    "vwr.barhScores(\"notLenet Accuracy with Traffic Signs from Wikipedia\", [notLenet_acc], [\"Accuracy\"])\n",
    "```\n",
    "\n",
    "![](./wup_assets/notLenetWikiAcc.png)\n",
    "\n",
    "While the model was able to achieve a 100% accuracy with the wikipedia signs, it only achieved a 96.5% accuracy with the Testing data (see [Accuracy Across All Datasets](#accuracy-across-all-datasets). The model correctly identified previously unseen Wikipedia signs, however, the clear symbols, colors, and shapes of the graphics probably allowed the model to more easily classify the them.\n",
    "\n",
    "![](./wup_assets/notLenetAllAcc.png)\n",
    "\n",
    "### Model Certainty and Softmax Probabilities\n",
    "\n",
    "Below are visualizations of the model's top-5 classifications for each of the Wikipedia signs. The model was very confident (~100%) for each classification. The classification confidence for the \"Bicycles Crossing\" sign, however, was ~70%. \n",
    "\n",
    "![](./wup_assets/notLenetWikiSoftmax.png)\n",
    "\n",
    "### A Closer Look at Bicycles Crossing Sign Classification\n",
    "\n",
    "The model was only ~70% confident in the classification of \"Bicycles Crossing\". It was ~30% confident that the sign was for \"Beware of Ice and Snow\". This may be explained by the fact that \"Beware of Ice and Snow\" was ranked as the #2 Hardest Sign for the model to recognize (see [Hardest Signs](#hardest-signs)) with a precision of only 87%. What is happening in this instance is the model tending to have a false positive (~13%) of the \"Beware of Ice and Snow\" sign.\n",
    "\n",
    "![](./wup_assets/notLenetWikiSoftmaxBicycles.png)\n",
    "\n",
    "## Visualize Layers of the Neural Network\n",
    "\n",
    "The image selected was a \"Speed Limit 70km/h\" sign. The `Model.eval_layer()` method is then used to plot and examine the layer output of the selected image.\n",
    "\n",
    "![](./wup_assets/70kmh.png)\n",
    "\n",
    "\n",
    "### Layer 0 Visualization\n",
    "\n",
    "Below shows the image of notLenet's Layer 0, which is a convolutional layer with a 32x32x1 output (1x1 filter). The activations provide appear to convert the sign to grayscale, with the contours of the red circular boundary and the number \"70\" visible.\n",
    "\n",
    "![](./wup_assets/Layer0.png)\n",
    "\n",
    "### Layer 1 Visualization\n",
    "\n",
    "The images of notLenet's Layer 1 convolutions (output shape 28x28x24) appear to be activating on the sign's circular shape and encoding the interior of the image with its representation of \"70\".\n",
    "\n",
    "![](./wup_assets/Layer1.png) \n",
    "\n",
    "## Build a Multiscale CNN\n",
    "\n",
    "Out of curiosity, a Multi-Scale CNN, similar to the one described by Sermanet and Lecunn in _\"Traffic Sign Recognition with Multi-Scale Convolutional Networks\"_. Unlike the notLenet model discussed earlier in this project, the Multi-Scale CNN is not sequential. \n",
    "\n",
    "### Concatenate Layer\n",
    "\n",
    "A custom layer class, `class Concatenate(Layer)`, wraps the Tensorflow function `concat()`. It is used to concatenate the output of the first layer with the later stages of the model.\n",
    "\n",
    "```python\n",
    "class Concatenate(Layer):\n",
    "    \n",
    "    def setName(self):\n",
    "        self.name = \"Concatenate\"\n",
    "        return\n",
    "\n",
    "    def connect(self, *prev_layers):\n",
    "        self.model = prev_layers[0].model\n",
    "        tensors = [layer.tensor for layer in prev_layers]\n",
    "        self.tensor = tf.concat(tensors, axis=1)\n",
    "        return self\n",
    "```\n",
    "\n",
    "### MutiScale Archictecture\n",
    "\n",
    "The model's architecture is similar to the one Sermanet and Lecunn described in _\"Traffic Sign Recognition with Multi-Scale Convolutional Networks\"_ where the output of the first stage is fed to the classifier stage.\n",
    "\n",
    "```python\n",
    "notSermanet = Model(\"notSermanet\", image_shape, n_classes)\n",
    "\n",
    "stage_1x1 = Conv2D ([32, 32, 1]).connect(notSermanet.inputLayer())\n",
    "\n",
    "stage_1 = Conv2D ([28, 28, 24]).connect(stage_1x1)\n",
    "stage_1 = Pooling([14, 14]).connect(stage_1)\n",
    "flatten_1 = Flatten().connect(stage_1)\n",
    "\n",
    "stage_2 = Conv2D ([10, 10, 64]).connect(stage_1)\n",
    "stage_2 = Pooling([ 5,  5]).connect(stage_2)\n",
    "stage_2 = Conv2D ([3, 3, 96]).connect(stage_2)\n",
    "flatten_2 = Flatten().connect(stage_2)\n",
    "\n",
    "stage_3 = Concatenate().connect(flatten_1, flatten_2)\n",
    "stage_3 = Connected([240]).connect(stage_3)\n",
    "stage_3 = Dropout().connect(stage_3)\n",
    "stage_3 = Connected([168]).connect(stage_3)\n",
    "stage_3 = Dropout().connect(stage_3)\n",
    "\n",
    "notSermanet.connectLogits(stage_3)\n",
    "```\n",
    "\n",
    "Visually:\n",
    "\n",
    "![](./wup_assets/multiscaleCNN.png) \n",
    "\n",
    "### Training\n",
    "\n",
    "notSermanet achieved 99.0% validation accuracy at epoch 42.\n",
    "\n",
    "![](./wup_assets/notSermanetValAcc.png) \n",
    "\n",
    "The model achieved a 97.1% accuracy with Testing data. Accuracy across all datasets are plotted below:\n",
    "\n",
    "![](./wup_assets/notSermanetAllAcc.png) \n",
    "\n",
    "### Most Difficult Signs\n",
    "\n",
    "notSermanet's top 10 most difficult signs are plotted below. Just like in notLenet, the \"Beware of Ice/Snow\", \"Pedestrians\", and \"Dangerous Curve to the Right\" are in the top of the group.\n",
    "\n",
    "![](./wup_assets/notSermanetHardest.png) \n",
    "\n",
    "### Most Easy Signs\n",
    "\n",
    "notSermanet's top 10 most easy signs are plotted below. Only the signs \"Go Straight or Right\", \"No Passing for Vehicles over 3.5 metric tons\", \"Dangerous Curve to the Left\", and \"Bumpy Road\" are in notSermanet's top 10, while are rest the signs are common to notLenet's.\n",
    "\n",
    "![](./wup_assets/notSermanetEasiest.png) \n",
    "\n",
    "### Accuracy with Wikipedia Signs\n",
    "\n",
    "Like notLenet, notSermanet correctly identifies all the Wikipedia signs (100% accuracy).\n",
    "\n",
    "![](./wup_assets/notSermanetWikiAcc.png)\n",
    "\n",
    "### Visualize Softmax Probabilties\n",
    "\n",
    "Also like notLenet, notSermanet has high confidence in all of its classifications. For the \"Bicycles Crossing\" sign, notSermanet did have a small ~1% confidence that it was a \"Beware of Ice/Snow\" sign.\n",
    "\n",
    "![](./wup_assets/notSermanetWikiSoftmax.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "mature-private",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorboard.notebook'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-19c3dad8e511>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'load_ext'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tensorboard.notebook'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\bcl\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2324\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2325\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2326\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2327\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-65>\u001b[0m in \u001b[0;36mload_ext\u001b[1;34m(self, module_str)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bcl\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bcl\\lib\\site-packages\\IPython\\core\\magics\\extension.py\u001b[0m in \u001b[0;36mload_ext\u001b[1;34m(self, module_str)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodule_str\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mUsageError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Missing module name.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextension_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_extension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'already loaded'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bcl\\lib\\site-packages\\IPython\\core\\extensions.py\u001b[0m in \u001b[0;36mload_extension\u001b[1;34m(self, module_str)\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmodule_str\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mprepended_to_syspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                     \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m                         print((\"Loading extensions from {dir} is deprecated. \"\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bcl\\lib\\importlib\\__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bcl\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bcl\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bcl\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorboard.notebook'"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard.notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mighty-noise",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "mysterious-haiti",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4cb0c574f0fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-4cb0c574f0fc>\u001b[0m in \u001b[0;36mcreate_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     '''\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLambda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m160\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m320\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCropping2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcropping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def create_model():\n",
    "    '''\n",
    "    Creates a model for use in autonmous driving mode. Architecture is \n",
    "    similar to the one in ref: Bojarski et. al., \"End to end Larning \n",
    "    for Self-Driving Cars\", 25APR2016.\n",
    "    '''\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(normalize, input_shape=(160,320,3)))\n",
    "    model.add(Cropping2D(cropping=[(50, 20), (0, 0)]))\n",
    "    \n",
    "    model.add(Conv2D(filters=24, kernel_size=5, strides=2, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    model.add(Conv2D(filters=36, kernel_size=5, strides=2, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    model.add(Conv2D(filters=48, kernel_size=5, strides=2, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv2D(filters=64, kernel_size=3, strides=1, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv2D(filters=64, kernel_size=3, strides=1, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(1))    \n",
    "    \n",
    "    model.compile(loss='MSE', optimizer='Adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.load('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-oliver",
   "metadata": {},
   "source": [
    "# Data Generator\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "respiratory-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil, floor\n",
    "from random import shuffle\n",
    "from keras.utils import Sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "theta       = 0.35\n",
    "corrections = {'center': 0, 'left': theta, 'right': -theta}\n",
    "\n",
    "dx          = 50\n",
    "shift_val   = {'left': dx, 'right': -dx}\n",
    "\n",
    "\n",
    "def shift(img, dx):\n",
    "        \n",
    "    shifted_img = img.copy()\n",
    "    if dx > 0:\n",
    "        # shift right\n",
    "        shifted_img[:,dx:] = img[:,:-dx]\n",
    "    else:\n",
    "        # shift left\n",
    "        shifted_img[:,:dx] = img[:,-dx:]\n",
    "\n",
    "    return shifted_img\n",
    "\n",
    "\n",
    "# ref: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "class DrivingLogSequence(Sequence):\n",
    "    \n",
    "    def __init__ (self, driving_log, batch_size=32):\n",
    "        \n",
    "        self.driving_log = driving_log\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def __len__(self):\n",
    "        return floor(len(self.driving_log)/self.batch_size)\n",
    "    \n",
    "    def add_data(self, img, steering):\n",
    "        self.images.append(img)\n",
    "        self.steerings.append([steering])\n",
    "        return\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        batch_driving_log = self.driving_log[index:index+self.batch_size]\n",
    "        self.images = []\n",
    "        self.steerings = []\n",
    "        \n",
    "        for line in batch_driving_log:\n",
    "            \n",
    "            center_steering = float(line['steering'])\n",
    "            \n",
    "            for camera in corrections:\n",
    "                \n",
    "                filename = line[camera]\n",
    "                \n",
    "                img = plt.imread(filename).copy()\n",
    "                steering = center_steering + corrections[camera]\n",
    "                \n",
    "                self.add_data(img,           steering)\n",
    "                self.add_data(np.fliplr(img), -steering)\n",
    "                \n",
    "                if camera != 'center':\n",
    "                    steering  = steering + corrections[camera]\n",
    "                    shift_img = shift(img, shift_val[camera])\n",
    "                    \n",
    "                    self.add_data(shift_img,            steering)\n",
    "                    self.add_data(np.fliplr(shift_img), -steering)\n",
    "            \n",
    "        X = np.array(self.images)\n",
    "        y = np.array(self.steerings)\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        shuffle(self.driving_log)\n",
    "        return\n",
    "    \n",
    "\n",
    "def create_generators(test_size=0.2, shuffle=True):\n",
    "    \n",
    "    drv_log_folders = ['data_easy_route','data_hard_route']\n",
    "    drv_log_filename = 'driving_log.csv'\n",
    "\n",
    "    drv_log = []\n",
    "    for drv_log_folder in drv_log_folders:\n",
    "        with open(drv_log_folder + '/' + drv_log_filename) as driving_log_file:\n",
    "            driving_log_reader = csv.DictReader(driving_log_file)\n",
    "            for line in driving_log_reader:\n",
    "                for camera in corrections:\n",
    "                    line[camera] = drv_log_folder + \"/\" + line[camera].strip()\n",
    "                drv_log.append(line)\n",
    "                \n",
    "    training, validation = train_test_split(drv_log, test_size=0.2, shuffle=True)                                       \n",
    "                       \n",
    "    return DrivingLogSequence(training), DrivingLogSequence(validation)        \n",
    "                       \n",
    "                       \n",
    "driving_log_seq_training, driving_log_seq_validation = create_generators()\n",
    "                       \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-atlas",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def OLD__getitem__OLD(self, index):\n",
    "        \n",
    "        batch_driving_log = self.driving_log[index:index+self.batch_size]\n",
    "        images = []\n",
    "        steerings = []\n",
    "        \n",
    "        for line in batch_driving_log:\n",
    "            \n",
    "            center_steering = float(line['steering'])\n",
    "            \n",
    "            for cam_pos, theta in cameras:\n",
    "                \n",
    "                filename = line[cam_pos].strip()\n",
    "                \n",
    "                img = plt.imread(filename).copy()\n",
    "                images.append(img)\n",
    "                steering = center_steering + theta\n",
    "                steerings.append([steering])\n",
    "                \n",
    "                # augment with flipped image\n",
    "                flip_img = np.fliplr(img)\n",
    "                images.append(flip_img)\n",
    "                steerings.append([-steering])\n",
    "                \n",
    "                if cam_pos == 'left':\n",
    "                    # augment with right shift\n",
    "                    rightshift_img = shift(img, 50)\n",
    "                    images.append(rightshift_img)\n",
    "                    steerings.append([steering + theta])\n",
    "\n",
    "                    # augment with flipped right shift image\n",
    "                    flip_img = np.fliplr(rightshift_img)\n",
    "                    images.append(flip_img)\n",
    "                    steerings.append([-(steering + theta)])\n",
    "                    \n",
    "                elif cam_pos == 'right':\n",
    "                    # augment with left shift\n",
    "                    leftshift_img = shift(img, -50)\n",
    "                    images.append(leftshift_img)\n",
    "                    steerings.append([steering - theta])\n",
    "\n",
    "                    # augment with flipped left shift image\n",
    "                    flip_img = np.fliplr(leftshift_img)\n",
    "                    images.append(flip_img)\n",
    "                    steerings.append([-(steering - theta)])\n",
    "            \n",
    "        X = np.array(images)\n",
    "        y = np.array(steerings)\n",
    "        \n",
    "        return X, y\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-essay",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "driving_log_filename = \n",
    "\n",
    "driving_log = []\n",
    "\n",
    "with open(driving_log_filename) as driving_log_file:\n",
    "    driving_log_reader = csv.DictReader(driving_log_file)\n",
    "    for line in driving_log_reader:\n",
    "        driving_log.append(line)\n",
    "        \n",
    "driving_log_filename = 'data/driving_log.csv'\n",
    "\n",
    "with open(driving_log_filename) as driving_log_file:\n",
    "    driving_log_reader = csv.DictReader(driving_log_file)\n",
    "    for line in driving_log_reader:\n",
    "        driving_log.append(line)\n",
    "        \n",
    "        \n",
    "driving_log_training, driving_log_validation = train_test_split(driving_log, test_size=0.2, shuffle=True)\n",
    "driving_log_seq_training   = DrivingLogSequence(driving_log_training)\n",
    "driving_log_seq_validation = DrivingLogSequence(driving_log_validation)        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-penetration",
   "metadata": {},
   "source": [
    "### Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-argentina",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_name = \"sample_data/IMG/left_2016_12_01_13_30_48_287.jpg\"\n",
    "img=plt.imread(\"sample_data/IMG/left_2016_12_01_13_30_48_287.jpg\")\n",
    "shift_img = shift(img, -50)\n",
    "plt.imshow(shift_img)\n",
    "\n",
    "#img_copy = np.copy(img)\n",
    "#img_copy[:,159:162] = [255, 0, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-strike",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def shift(img, dx):\n",
    "\n",
    "    shifted_img = np.zeros_like(img)\n",
    "    if dx > 0:\n",
    "        # shift right\n",
    "        shifted_img[:,dx:] = img[:,:-dx]\n",
    "    else:\n",
    "        # shift left\n",
    "        shifted_img[:,:dx] = img[:,-dx:]\n",
    "\n",
    "    return shifted_img\n",
    "\n",
    "shift_img = shift(img, -75)\n",
    "plt.imshow(shift_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-glenn",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-quest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "driving_log_filename = 'sample_data/driving_log.csv'\n",
    "\n",
    "center_images = []\n",
    "steerings = []\n",
    "with open(driving_log_filename) as driving_log:\n",
    "    driving_log_reader = csv.DictReader(driving_log)\n",
    "    for row in driving_log_reader:\n",
    "        center_images.append(plt.imread('sample_data/' + row['center']))\n",
    "        steerings.append([float(row['steering'])])\n",
    "                            \n",
    "X_train = np.array(center_images)\n",
    "y_train = np.array(steerings)\n",
    "    \n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-civilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding left and right cameras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "driving_log_filename = 'sample_data/driving_log.csv'\n",
    "\n",
    "images = []\n",
    "steerings = []\n",
    "theta = 0.35\n",
    "with open(driving_log_filename) as driving_log:\n",
    "    driving_log_reader = csv.DictReader(driving_log)\n",
    "    for row in driving_log_reader:\n",
    "        \n",
    "        steering = float(row['steering'])\n",
    "        \n",
    "        images.append(plt.imread('sample_data/' + row['center'].strip()))\n",
    "        steerings.append([steering])\n",
    "        \n",
    "        images.append(plt.imread('sample_data/' + row['left'].strip()))\n",
    "        steerings.append([steering + theta])\n",
    "        \n",
    "        images.append(plt.imread('sample_data/' + row['right'].strip()))\n",
    "        steerings.append([steering - theta])\n",
    "        \n",
    "                            \n",
    "X_train = np.array(images)\n",
    "y_train = np.array(steerings)\n",
    "    \n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-gospel",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "'''\n",
    "def generator(lines, batch_size=32):\n",
    "\n",
    "    driving_log_filename = 'sample_data/driving_log.csv'\n",
    "\n",
    "    images = []\n",
    "    steerings = []\n",
    "    theta = 0.35\n",
    "    \n",
    "    with open(driving_log_filename) as driving_log:\n",
    "        num_lines = 0\n",
    "        for line in driving_log:\n",
    "            num_lines += 1\n",
    "\n",
    "    with open(driving_log_filename) as driving_log:\n",
    "\n",
    "        driving_log_reader = csv.DictReader(driving_log)\n",
    "        \n",
    "        \n",
    "        print (len(driving_log_reader))\n",
    "\n",
    "    for row in driving_log_reader:\n",
    "\n",
    "        steering = float(row['steering'])\n",
    "\n",
    "        images.append(plt.imread('sample_data/' + row['center'].strip()))\n",
    "        steerings.append([steering])\n",
    "\n",
    "        images.append(plt.imread('sample_data/' + row['left'].strip()))\n",
    "        steerings.append([steering + theta])\n",
    "\n",
    "        images.append(plt.imread('sample_data/' + row['right'].strip()))\n",
    "        steerings.append([steering - theta])\n",
    "    '''\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-effects",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[46]\n",
    "\n",
    "for y in y_train:\n",
    "    if y > 0:\n",
    "        print(y)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-haiti",
   "metadata": {},
   "source": [
    "# Network Structure\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "equipped-standard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:497: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:497: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3636: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3636: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3019: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3019: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1264: calling reduce_prod_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1264: calling reduce_prod_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_1 (Lambda)            (None, 160, 320, 3)       0         \n",
      "_________________________________________________________________\n",
      "cropping2d_1 (Cropping2D)    (None, 90, 320, 3)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 43, 158, 24)       1824      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 43, 158, 24)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, 77, 36)        21636     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 20, 77, 36)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 37, 48)         43248     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 37, 48)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 6, 35, 64)         27712     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 6, 35, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 4, 33, 64)         36928     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4, 33, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8448)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               844900    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 981,819\n",
      "Trainable params: 981,819\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Lambda, BatchNormalization, Flatten, Dense \n",
    "from keras.layers import Conv2D, Cropping2D, Dropout, MaxPooling2D\n",
    "import tensorflow as tf\n",
    "\n",
    "def normalize(rgb):\n",
    "    '''\n",
    "    Normalizes rgb image between [-1, 1].\n",
    "    Used in Lambda layer of model.\n",
    "    '''\n",
    "    return (rgb-128.0) / 128.0\n",
    "\n",
    "def create_model():\n",
    "    '''\n",
    "    Creates a model for use in autonmous driving mode. Architecture is \n",
    "    similar to the one in ref: Bojarski et. al., \"End to end Larning \n",
    "    for Self-Driving Cars\", 25APR2016.\n",
    "    '''\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(normalize, input_shape=(160,320,3)))\n",
    "    model.add(Cropping2D(cropping=[(50, 20), (0, 0)]))\n",
    "    \n",
    "    model.add(Conv2D(filters=24, kernel_size=5, strides=2, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    model.add(Conv2D(filters=36, kernel_size=5, strides=2, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    model.add(Conv2D(filters=48, kernel_size=5, strides=2, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv2D(filters=64, kernel_size=3, strides=1, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv2D(filters=64, kernel_size=3, strides=1, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(1))    \n",
    "    \n",
    "    model.compile(loss='MSE', optimizer='Adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "innocent-casting",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:64: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:64: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:497: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:497: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3636: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3636: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3019: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3019: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1264: calling reduce_prod_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1264: calling reduce_prod_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nmodel = Sequential()\\nmodel.add(Lambda(normalize, input_shape=(160,320,3)))\\nmodel.add(Cropping2D(cropping=[(50, 20), (0, 0)]))\\nmodel.add(Conv2D(filters=24, kernel_size=5, strides=2, activation='relu'))\\nmodel.add(Dropout(0.2))\\nmodel.add(BatchNormalization())\\nmodel.add(Conv2D(filters=36, kernel_size=5, strides=2, activation='relu'))\\nmodel.add(Dropout(0.2))\\nmodel.add(BatchNormalization())\\nmodel.add(Conv2D(filters=48, kernel_size=5, strides=2, activation='relu'))\\nmodel.add(BatchNormalization())\\nmodel.add(Conv2D(filters=64, kernel_size=3, strides=1, activation='relu'))\\nmodel.add(BatchNormalization())\\nmodel.add(Conv2D(filters=64, kernel_size=3, strides=1, activation='relu'))\\nmodel.add(BatchNormalization())\\nmodel.add(Flatten())\\nmodel.add(Dense(100, activation='relu'))\\nmodel.add(Dropout(0.5))\\nmodel.add(BatchNormalization())\\nmodel.add(Dense(50, activation='relu'))\\nmodel.add(Dropout(0.5))\\nmodel.add(BatchNormalization())\\nmodel.add(Dense(10, activation='relu'))\\nmodel.add(Dense(1))\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Lambda, BatchNormalization, Flatten, Dense \n",
    "from keras.layers import Conv2D, Cropping2D, Dropout, MaxPooling2D\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def normalize(rgb):\n",
    "    '''\n",
    "    normalize rgb between [-1, 1]\n",
    "    '''\n",
    "    \n",
    "    return (rgb-128.0) / 128.0\n",
    "\n",
    "\n",
    "# ---loss: 0.0175\n",
    "model = Sequential()\n",
    "model.add(Lambda(normalize, input_shape=(160,320,3)))\n",
    "model.add(Cropping2D(cropping=[(50, 20), (0, 0)]))\n",
    "model.add(Conv2D(filters=24, kernel_size=5, strides=2, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Conv2D(filters=36, kernel_size=5, strides=2, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Conv2D(filters=48, kernel_size=5, strides=2, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(filters=64, kernel_size=3, strides=1, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(filters=64, kernel_size=3, strides=1, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "'''\n",
    "model = Sequential()\n",
    "model.add(Lambda(normalize, input_shape=(160,320,3)))\n",
    "model.add(Cropping2D(cropping=[(50, 20), (0, 0)]))\n",
    "model.add(Conv2D(filters=24, kernel_size=5, strides=2, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=36, kernel_size=5, strides=2, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=48, kernel_size=5, strides=2, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=64, kernel_size=3, strides=1, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=64, kernel_size=3, strides=1, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "'''\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "amber-cattle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_10 (Batc (None, 160, 320, 3)       12        \n",
      "_________________________________________________________________\n",
      "cropping2d_2 (Cropping2D)    (None, 90, 320, 3)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 43, 158, 24)       1824      \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 43, 158, 24)       96        \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 20, 77, 36)        21636     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 20, 77, 36)        144       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 8, 37, 48)         43248     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 8, 37, 48)         192       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 6, 35, 64)         27712     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 6, 35, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 4, 33, 64)         36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 4, 33, 64)         256       \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8448)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               844900    \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 983,415\n",
      "Trainable params: 982,617\n",
      "Non-trainable params: 798\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "comparable-public",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='MSE', optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "close-thong",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:958: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:958: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:680: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:680: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:945: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:945: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2378: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2378: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:159: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:159: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:164: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:164: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:173: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:173: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:182: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:182: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:189: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:189: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387/387 [==============================] - 142s 367ms/step - loss: 0.0621 - val_loss: 0.1012\n",
      "Epoch 2/30\n",
      "387/387 [==============================] - 130s 336ms/step - loss: 0.0185 - val_loss: 0.0667\n",
      "Epoch 3/30\n",
      "387/387 [==============================] - 131s 339ms/step - loss: 0.0104 - val_loss: 0.0569\n",
      "Epoch 4/30\n",
      "387/387 [==============================] - 131s 339ms/step - loss: 0.0088 - val_loss: 0.0623\n",
      "Epoch 5/30\n",
      "387/387 [==============================] - 130s 336ms/step - loss: 0.0080 - val_loss: 0.0479\n",
      "Epoch 6/30\n",
      "387/387 [==============================] - 130s 337ms/step - loss: 0.0065 - val_loss: 0.0447\n",
      "Epoch 7/30\n",
      "387/387 [==============================] - 133s 344ms/step - loss: 0.0079 - val_loss: 0.0650\n",
      "Epoch 8/30\n",
      "387/387 [==============================] - 132s 341ms/step - loss: 0.0060 - val_loss: 0.0350\n",
      "Epoch 9/30\n",
      "387/387 [==============================] - 128s 332ms/step - loss: 0.0066 - val_loss: 0.0370\n",
      "Epoch 10/30\n",
      "387/387 [==============================] - 132s 340ms/step - loss: 0.0050 - val_loss: 0.0457\n",
      "Epoch 11/30\n",
      "387/387 [==============================] - 131s 339ms/step - loss: 0.0047 - val_loss: 0.0362\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# https://keras.io/api/callbacks/early_stopping/\n",
    "early_stopper = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "model.fit_generator(driving_log_seq_training, \n",
    "                    validation_data=driving_log_seq_validation, \n",
    "                    epochs=30,\n",
    "                    callbacks=[early_stopper])\n",
    "\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "unlimited-worker",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-central",
   "metadata": {},
   "source": [
    "### Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-island",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, batch_size=512, initial_epoch=0, epochs=4, shuffle=True, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-bicycle",
   "metadata": {},
   "source": [
    "# drive.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-balance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import base64\n",
    "from datetime import datetime\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import socketio\n",
    "import eventlet\n",
    "import eventlet.wsgi\n",
    "from PIL import Image\n",
    "from flask import Flask\n",
    "from io import BytesIO\n",
    "\n",
    "from keras.models import load_model\n",
    "import h5py\n",
    "from keras import __version__ as keras_version\n",
    "\n",
    "sio = socketio.Server()\n",
    "app = Flask(__name__)\n",
    "model = None\n",
    "prev_image_array = None\n",
    "\n",
    "\n",
    "class SimplePIController:\n",
    "    def __init__(self, Kp, Ki):\n",
    "        self.Kp = Kp\n",
    "        self.Ki = Ki\n",
    "        self.set_point = 0.\n",
    "        self.error = 0.\n",
    "        self.integral = 0.\n",
    "\n",
    "    def set_desired(self, desired):\n",
    "        self.set_point = desired\n",
    "\n",
    "    def update(self, measurement):\n",
    "        # proportional error\n",
    "        self.error = self.set_point - measurement\n",
    "\n",
    "        # integral error\n",
    "        self.integral += self.error\n",
    "\n",
    "        return self.Kp * self.error + self.Ki * self.integral\n",
    "\n",
    "\n",
    "controller = SimplePIController(0.1, 0.002)\n",
    "set_speed = 9\n",
    "controller.set_desired(set_speed)\n",
    "\n",
    "\n",
    "@sio.on('telemetry')\n",
    "def telemetry(sid, data):\n",
    "    if data:\n",
    "        # The current steering angle of the car\n",
    "        steering_angle = data[\"steering_angle\"]\n",
    "        # The current throttle of the car\n",
    "        throttle = data[\"throttle\"]\n",
    "        # The current speed of the car\n",
    "        speed = data[\"speed\"]\n",
    "        # The current image from the center camera of the car\n",
    "        imgString = data[\"image\"]\n",
    "        image = Image.open(BytesIO(base64.b64decode(imgString)))\n",
    "        image_array = np.asarray(image).copy()\n",
    "        image_array[:,159:162] = [255, 0, 0]                \n",
    "        steering_angle = float(model.predict(image_array[None, :, :, :], batch_size=1))\n",
    "\n",
    "        throttle = controller.update(float(speed))\n",
    "\n",
    "        print(steering_angle, throttle)\n",
    "        send_control(steering_angle, throttle)\n",
    "\n",
    "        # save frame\n",
    "        if args.image_folder != '':\n",
    "            timestamp = datetime.utcnow().strftime('%Y_%m_%d_%H_%M_%S_%f')[:-3]\n",
    "            image_filename = os.path.join(args.image_folder, timestamp)\n",
    "            image.save('{}.jpg'.format(image_filename))\n",
    "    else:\n",
    "        # NOTE: DON'T EDIT THIS.\n",
    "        sio.emit('manual', data={}, skip_sid=True)\n",
    "\n",
    "\n",
    "@sio.on('connect')\n",
    "def connect(sid, environ):\n",
    "    print(\"connect \", sid)\n",
    "    send_control(0, 0)\n",
    "\n",
    "\n",
    "def send_control(steering_angle, throttle):\n",
    "    sio.emit(\n",
    "        \"steer\",\n",
    "        data={\n",
    "            'steering_angle': steering_angle.__str__(),\n",
    "            'throttle': throttle.__str__()\n",
    "        },\n",
    "        skip_sid=True)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Remote Driving')\n",
    "    parser.add_argument(\n",
    "        'model',\n",
    "        type=str,\n",
    "        help='Path to model h5 file. Model should be on the same path.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        'image_folder',\n",
    "        type=str,\n",
    "        nargs='?',\n",
    "        default='',\n",
    "        help='Path to image folder. This is where the images from the run will be saved.'\n",
    "    )\n",
    "    #parser.parse_args()    \n",
    "    #parser.parse_args(['--sum', '7', '-1', '42'])    \n",
    "    args = parser.parse_args(['model.h5'])\n",
    "\n",
    "    # check that model Keras version is same as local Keras version\n",
    "    f = h5py.File(args.model, mode='r')\n",
    "    model_version = f.attrs.get('keras_version')\n",
    "    keras_version = str(keras_version).encode('utf8')\n",
    "\n",
    "    if model_version != keras_version:\n",
    "        print('You are using Keras version ', keras_version,\n",
    "              ', but the model was built using ', model_version)\n",
    "\n",
    "    #mine\n",
    "    print(args.model)\n",
    "    model = load_model(args.model)\n",
    "\n",
    "    if args.image_folder != '':\n",
    "        print(\"Creating image folder at {}\".format(args.image_folder))\n",
    "        if not os.path.exists(args.image_folder):\n",
    "            os.makedirs(args.image_folder)\n",
    "        else:\n",
    "            shutil.rmtree(args.image_folder)\n",
    "            os.makedirs(args.image_folder)\n",
    "        print(\"RECORDING THIS RUN ...\")\n",
    "    else:\n",
    "        print(\"NOT RECORDING THIS RUN ...\")\n",
    "\n",
    "    # wrap Flask application with engineio's middleware\n",
    "    app = socketio.Middleware(sio, app)\n",
    "\n",
    "    # deploy as an eventlet WSGI server\n",
    "    eventlet.wsgi.server(eventlet.listen(('', 4567)), app)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-milwaukee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-strike",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socketio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bcl] *",
   "language": "python",
   "name": "conda-env-bcl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
