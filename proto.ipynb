{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "mature-private",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorboard.notebook'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-19c3dad8e511>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'load_ext'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tensorboard.notebook'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\bcl\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2324\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2325\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2326\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2327\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-65>\u001b[0m in \u001b[0;36mload_ext\u001b[1;34m(self, module_str)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bcl\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bcl\\lib\\site-packages\\IPython\\core\\magics\\extension.py\u001b[0m in \u001b[0;36mload_ext\u001b[1;34m(self, module_str)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodule_str\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mUsageError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Missing module name.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextension_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_extension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'already loaded'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bcl\\lib\\site-packages\\IPython\\core\\extensions.py\u001b[0m in \u001b[0;36mload_extension\u001b[1;34m(self, module_str)\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmodule_str\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mprepended_to_syspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                     \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m                         print((\"Loading extensions from {dir} is deprecated. \"\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bcl\\lib\\importlib\\__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bcl\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bcl\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bcl\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorboard.notebook'"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard.notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mighty-noise",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-oliver",
   "metadata": {},
   "source": [
    "# Data Generator\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "respiratory-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil, floor\n",
    "from random import shuffle\n",
    "from keras.utils import Sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "theta       = 0.35\n",
    "corrections = {'center': 0, 'left': theta, 'right': -theta}\n",
    "\n",
    "dx          = 50\n",
    "shift_val   = {'left': dx, 'right': -dx}\n",
    "\n",
    "\n",
    "def shift(img, dx):\n",
    "        \n",
    "    shifted_img = img.copy()\n",
    "    if dx > 0:\n",
    "        # shift right\n",
    "        shifted_img[:,dx:] = img[:,:-dx]\n",
    "    else:\n",
    "        # shift left\n",
    "        shifted_img[:,:dx] = img[:,-dx:]\n",
    "\n",
    "    return shifted_img\n",
    "\n",
    "\n",
    "# ref: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "class DrivingLogSequence(Sequence):\n",
    "    \n",
    "    def __init__ (self, driving_log, batch_size=32):\n",
    "        \n",
    "        self.driving_log = driving_log\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def __len__(self):\n",
    "        return floor(len(self.driving_log)/self.batch_size)\n",
    "    \n",
    "    def add_data(self, img, steering):\n",
    "        self.images.append(img)\n",
    "        self.steerings.append([steering])\n",
    "        return\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        batch_driving_log = self.driving_log[index:index+self.batch_size]\n",
    "        self.images = []\n",
    "        self.steerings = []\n",
    "        \n",
    "        for line in batch_driving_log:\n",
    "            \n",
    "            center_steering = float(line['steering'])\n",
    "            \n",
    "            for camera in corrections:\n",
    "                \n",
    "                filename = line[camera]\n",
    "                \n",
    "                img = plt.imread(filename).copy()\n",
    "                steering = center_steering + corrections[camera]\n",
    "                \n",
    "                self.add_data(img,           steering)\n",
    "                self.add_data(np.fliplr(img), -steering)\n",
    "                \n",
    "                if camera != 'center':\n",
    "                    steering  = steering + corrections[camera]\n",
    "                    shift_img = shift(img, shift_val[camera])\n",
    "                    \n",
    "                    self.add_data(shift_img,            steering)\n",
    "                    self.add_data(np.fliplr(shift_img), -steering)\n",
    "            \n",
    "        X = np.array(self.images)\n",
    "        y = np.array(self.steerings)\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        shuffle(self.driving_log)\n",
    "        return\n",
    "    \n",
    "\n",
    "def create_generators(test_size=0.2, shuffle=True):\n",
    "    \n",
    "    drv_log_folders = ['data_easy_route','data_hard_route']\n",
    "    drv_log_filename = 'driving_log.csv'\n",
    "\n",
    "    drv_log = []\n",
    "    for drv_log_folder in drv_log_folders:\n",
    "        with open(drv_log_folder + '/' + drv_log_filename) as driving_log_file:\n",
    "            driving_log_reader = csv.DictReader(driving_log_file)\n",
    "            for line in driving_log_reader:\n",
    "                for camera in corrections:\n",
    "                    line[camera] = drv_log_folder + \"/\" + line[camera].strip()\n",
    "                drv_log.append(line)\n",
    "                \n",
    "    training, validation = train_test_split(drv_log, test_size=0.2, shuffle=True)                                       \n",
    "                       \n",
    "    return DrivingLogSequence(training), DrivingLogSequence(validation)        \n",
    "                       \n",
    "                       \n",
    "driving_log_seq_training, driving_log_seq_validation = create_generators()\n",
    "                       \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-atlas",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def OLD__getitem__OLD(self, index):\n",
    "        \n",
    "        batch_driving_log = self.driving_log[index:index+self.batch_size]\n",
    "        images = []\n",
    "        steerings = []\n",
    "        \n",
    "        for line in batch_driving_log:\n",
    "            \n",
    "            center_steering = float(line['steering'])\n",
    "            \n",
    "            for cam_pos, theta in cameras:\n",
    "                \n",
    "                filename = line[cam_pos].strip()\n",
    "                \n",
    "                img = plt.imread(filename).copy()\n",
    "                images.append(img)\n",
    "                steering = center_steering + theta\n",
    "                steerings.append([steering])\n",
    "                \n",
    "                # augment with flipped image\n",
    "                flip_img = np.fliplr(img)\n",
    "                images.append(flip_img)\n",
    "                steerings.append([-steering])\n",
    "                \n",
    "                if cam_pos == 'left':\n",
    "                    # augment with right shift\n",
    "                    rightshift_img = shift(img, 50)\n",
    "                    images.append(rightshift_img)\n",
    "                    steerings.append([steering + theta])\n",
    "\n",
    "                    # augment with flipped right shift image\n",
    "                    flip_img = np.fliplr(rightshift_img)\n",
    "                    images.append(flip_img)\n",
    "                    steerings.append([-(steering + theta)])\n",
    "                    \n",
    "                elif cam_pos == 'right':\n",
    "                    # augment with left shift\n",
    "                    leftshift_img = shift(img, -50)\n",
    "                    images.append(leftshift_img)\n",
    "                    steerings.append([steering - theta])\n",
    "\n",
    "                    # augment with flipped left shift image\n",
    "                    flip_img = np.fliplr(leftshift_img)\n",
    "                    images.append(flip_img)\n",
    "                    steerings.append([-(steering - theta)])\n",
    "            \n",
    "        X = np.array(images)\n",
    "        y = np.array(steerings)\n",
    "        \n",
    "        return X, y\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-essay",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "driving_log_filename = \n",
    "\n",
    "driving_log = []\n",
    "\n",
    "with open(driving_log_filename) as driving_log_file:\n",
    "    driving_log_reader = csv.DictReader(driving_log_file)\n",
    "    for line in driving_log_reader:\n",
    "        driving_log.append(line)\n",
    "        \n",
    "driving_log_filename = 'data/driving_log.csv'\n",
    "\n",
    "with open(driving_log_filename) as driving_log_file:\n",
    "    driving_log_reader = csv.DictReader(driving_log_file)\n",
    "    for line in driving_log_reader:\n",
    "        driving_log.append(line)\n",
    "        \n",
    "        \n",
    "driving_log_training, driving_log_validation = train_test_split(driving_log, test_size=0.2, shuffle=True)\n",
    "driving_log_seq_training   = DrivingLogSequence(driving_log_training)\n",
    "driving_log_seq_validation = DrivingLogSequence(driving_log_validation)        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-penetration",
   "metadata": {},
   "source": [
    "### Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-argentina",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_name = \"sample_data/IMG/left_2016_12_01_13_30_48_287.jpg\"\n",
    "img=plt.imread(\"sample_data/IMG/left_2016_12_01_13_30_48_287.jpg\")\n",
    "shift_img = shift(img, -50)\n",
    "plt.imshow(shift_img)\n",
    "\n",
    "#img_copy = np.copy(img)\n",
    "#img_copy[:,159:162] = [255, 0, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-strike",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def shift(img, dx):\n",
    "\n",
    "    shifted_img = np.zeros_like(img)\n",
    "    if dx > 0:\n",
    "        # shift right\n",
    "        shifted_img[:,dx:] = img[:,:-dx]\n",
    "    else:\n",
    "        # shift left\n",
    "        shifted_img[:,:dx] = img[:,-dx:]\n",
    "\n",
    "    return shifted_img\n",
    "\n",
    "shift_img = shift(img, -75)\n",
    "plt.imshow(shift_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-glenn",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-quest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "driving_log_filename = 'sample_data/driving_log.csv'\n",
    "\n",
    "center_images = []\n",
    "steerings = []\n",
    "with open(driving_log_filename) as driving_log:\n",
    "    driving_log_reader = csv.DictReader(driving_log)\n",
    "    for row in driving_log_reader:\n",
    "        center_images.append(plt.imread('sample_data/' + row['center']))\n",
    "        steerings.append([float(row['steering'])])\n",
    "                            \n",
    "X_train = np.array(center_images)\n",
    "y_train = np.array(steerings)\n",
    "    \n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-civilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding left and right cameras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "driving_log_filename = 'sample_data/driving_log.csv'\n",
    "\n",
    "images = []\n",
    "steerings = []\n",
    "theta = 0.35\n",
    "with open(driving_log_filename) as driving_log:\n",
    "    driving_log_reader = csv.DictReader(driving_log)\n",
    "    for row in driving_log_reader:\n",
    "        \n",
    "        steering = float(row['steering'])\n",
    "        \n",
    "        images.append(plt.imread('sample_data/' + row['center'].strip()))\n",
    "        steerings.append([steering])\n",
    "        \n",
    "        images.append(plt.imread('sample_data/' + row['left'].strip()))\n",
    "        steerings.append([steering + theta])\n",
    "        \n",
    "        images.append(plt.imread('sample_data/' + row['right'].strip()))\n",
    "        steerings.append([steering - theta])\n",
    "        \n",
    "                            \n",
    "X_train = np.array(images)\n",
    "y_train = np.array(steerings)\n",
    "    \n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-gospel",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "'''\n",
    "def generator(lines, batch_size=32):\n",
    "\n",
    "    driving_log_filename = 'sample_data/driving_log.csv'\n",
    "\n",
    "    images = []\n",
    "    steerings = []\n",
    "    theta = 0.35\n",
    "    \n",
    "    with open(driving_log_filename) as driving_log:\n",
    "        num_lines = 0\n",
    "        for line in driving_log:\n",
    "            num_lines += 1\n",
    "\n",
    "    with open(driving_log_filename) as driving_log:\n",
    "\n",
    "        driving_log_reader = csv.DictReader(driving_log)\n",
    "        \n",
    "        \n",
    "        print (len(driving_log_reader))\n",
    "\n",
    "    for row in driving_log_reader:\n",
    "\n",
    "        steering = float(row['steering'])\n",
    "\n",
    "        images.append(plt.imread('sample_data/' + row['center'].strip()))\n",
    "        steerings.append([steering])\n",
    "\n",
    "        images.append(plt.imread('sample_data/' + row['left'].strip()))\n",
    "        steerings.append([steering + theta])\n",
    "\n",
    "        images.append(plt.imread('sample_data/' + row['right'].strip()))\n",
    "        steerings.append([steering - theta])\n",
    "    '''\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-effects",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[46]\n",
    "\n",
    "for y in y_train:\n",
    "    if y > 0:\n",
    "        print(y)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-haiti",
   "metadata": {},
   "source": [
    "# Network Structure\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "innocent-casting",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:64: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:64: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:497: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:497: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3636: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3636: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3019: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3019: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1264: calling reduce_prod_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1264: calling reduce_prod_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nmodel = Sequential()\\nmodel.add(Lambda(normalize, input_shape=(160,320,3)))\\nmodel.add(Cropping2D(cropping=[(50, 20), (0, 0)]))\\nmodel.add(Conv2D(filters=24, kernel_size=5, strides=2, activation='relu'))\\nmodel.add(Dropout(0.2))\\nmodel.add(BatchNormalization())\\nmodel.add(Conv2D(filters=36, kernel_size=5, strides=2, activation='relu'))\\nmodel.add(Dropout(0.2))\\nmodel.add(BatchNormalization())\\nmodel.add(Conv2D(filters=48, kernel_size=5, strides=2, activation='relu'))\\nmodel.add(BatchNormalization())\\nmodel.add(Conv2D(filters=64, kernel_size=3, strides=1, activation='relu'))\\nmodel.add(BatchNormalization())\\nmodel.add(Conv2D(filters=64, kernel_size=3, strides=1, activation='relu'))\\nmodel.add(BatchNormalization())\\nmodel.add(Flatten())\\nmodel.add(Dense(100, activation='relu'))\\nmodel.add(Dropout(0.5))\\nmodel.add(BatchNormalization())\\nmodel.add(Dense(50, activation='relu'))\\nmodel.add(Dropout(0.5))\\nmodel.add(BatchNormalization())\\nmodel.add(Dense(10, activation='relu'))\\nmodel.add(Dense(1))\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Lambda, BatchNormalization, Flatten, Dense \n",
    "from keras.layers import Conv2D, Cropping2D, Dropout, MaxPooling2D\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def normalize(rgb):\n",
    "    '''\n",
    "    normalize rgb between [-1, 1]\n",
    "    '''\n",
    "    \n",
    "    return (rgb-128.0) / 128.0\n",
    "\n",
    "\n",
    "# ---loss: 0.0175\n",
    "model = Sequential()\n",
    "model.add(Lambda(normalize, input_shape=(160,320,3)))\n",
    "model.add(Cropping2D(cropping=[(50, 20), (0, 0)]))\n",
    "model.add(Conv2D(filters=24, kernel_size=5, strides=2, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Conv2D(filters=36, kernel_size=5, strides=2, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Conv2D(filters=48, kernel_size=5, strides=2, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(filters=64, kernel_size=3, strides=1, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(filters=64, kernel_size=3, strides=1, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "'''\n",
    "model = Sequential()\n",
    "model.add(Lambda(normalize, input_shape=(160,320,3)))\n",
    "model.add(Cropping2D(cropping=[(50, 20), (0, 0)]))\n",
    "model.add(Conv2D(filters=24, kernel_size=5, strides=2, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=36, kernel_size=5, strides=2, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=48, kernel_size=5, strides=2, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=64, kernel_size=3, strides=1, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=64, kernel_size=3, strides=1, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "'''\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "amber-cattle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_10 (Batc (None, 160, 320, 3)       12        \n",
      "_________________________________________________________________\n",
      "cropping2d_2 (Cropping2D)    (None, 90, 320, 3)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 43, 158, 24)       1824      \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 43, 158, 24)       96        \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 20, 77, 36)        21636     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 20, 77, 36)        144       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 8, 37, 48)         43248     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 8, 37, 48)         192       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 6, 35, 64)         27712     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 6, 35, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 4, 33, 64)         36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 4, 33, 64)         256       \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8448)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               844900    \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 983,415\n",
      "Trainable params: 982,617\n",
      "Non-trainable params: 798\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "comparable-public",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='MSE', optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "close-thong",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:958: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:958: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:680: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:680: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:945: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:945: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2378: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2378: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:159: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:159: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:164: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:164: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:173: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:173: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:182: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:182: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:189: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:189: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387/387 [==============================] - 142s 367ms/step - loss: 0.0621 - val_loss: 0.1012\n",
      "Epoch 2/30\n",
      "387/387 [==============================] - 130s 336ms/step - loss: 0.0185 - val_loss: 0.0667\n",
      "Epoch 3/30\n",
      "387/387 [==============================] - 131s 339ms/step - loss: 0.0104 - val_loss: 0.0569\n",
      "Epoch 4/30\n",
      "387/387 [==============================] - 131s 339ms/step - loss: 0.0088 - val_loss: 0.0623\n",
      "Epoch 5/30\n",
      "387/387 [==============================] - 130s 336ms/step - loss: 0.0080 - val_loss: 0.0479\n",
      "Epoch 6/30\n",
      "387/387 [==============================] - 130s 337ms/step - loss: 0.0065 - val_loss: 0.0447\n",
      "Epoch 7/30\n",
      "387/387 [==============================] - 133s 344ms/step - loss: 0.0079 - val_loss: 0.0650\n",
      "Epoch 8/30\n",
      "387/387 [==============================] - 132s 341ms/step - loss: 0.0060 - val_loss: 0.0350\n",
      "Epoch 9/30\n",
      "387/387 [==============================] - 128s 332ms/step - loss: 0.0066 - val_loss: 0.0370\n",
      "Epoch 10/30\n",
      "387/387 [==============================] - 132s 340ms/step - loss: 0.0050 - val_loss: 0.0457\n",
      "Epoch 11/30\n",
      "387/387 [==============================] - 131s 339ms/step - loss: 0.0047 - val_loss: 0.0362\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# https://keras.io/api/callbacks/early_stopping/\n",
    "early_stopper = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "model.fit_generator(driving_log_seq_training, \n",
    "                    validation_data=driving_log_seq_validation, \n",
    "                    epochs=30,\n",
    "                    callbacks=[early_stopper])\n",
    "\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "unlimited-worker",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-central",
   "metadata": {},
   "source": [
    "### Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-island",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, batch_size=512, initial_epoch=0, epochs=4, shuffle=True, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-bicycle",
   "metadata": {},
   "source": [
    "# drive.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-balance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import base64\n",
    "from datetime import datetime\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import socketio\n",
    "import eventlet\n",
    "import eventlet.wsgi\n",
    "from PIL import Image\n",
    "from flask import Flask\n",
    "from io import BytesIO\n",
    "\n",
    "from keras.models import load_model\n",
    "import h5py\n",
    "from keras import __version__ as keras_version\n",
    "\n",
    "sio = socketio.Server()\n",
    "app = Flask(__name__)\n",
    "model = None\n",
    "prev_image_array = None\n",
    "\n",
    "\n",
    "class SimplePIController:\n",
    "    def __init__(self, Kp, Ki):\n",
    "        self.Kp = Kp\n",
    "        self.Ki = Ki\n",
    "        self.set_point = 0.\n",
    "        self.error = 0.\n",
    "        self.integral = 0.\n",
    "\n",
    "    def set_desired(self, desired):\n",
    "        self.set_point = desired\n",
    "\n",
    "    def update(self, measurement):\n",
    "        # proportional error\n",
    "        self.error = self.set_point - measurement\n",
    "\n",
    "        # integral error\n",
    "        self.integral += self.error\n",
    "\n",
    "        return self.Kp * self.error + self.Ki * self.integral\n",
    "\n",
    "\n",
    "controller = SimplePIController(0.1, 0.002)\n",
    "set_speed = 9\n",
    "controller.set_desired(set_speed)\n",
    "\n",
    "\n",
    "@sio.on('telemetry')\n",
    "def telemetry(sid, data):\n",
    "    if data:\n",
    "        # The current steering angle of the car\n",
    "        steering_angle = data[\"steering_angle\"]\n",
    "        # The current throttle of the car\n",
    "        throttle = data[\"throttle\"]\n",
    "        # The current speed of the car\n",
    "        speed = data[\"speed\"]\n",
    "        # The current image from the center camera of the car\n",
    "        imgString = data[\"image\"]\n",
    "        image = Image.open(BytesIO(base64.b64decode(imgString)))\n",
    "        image_array = np.asarray(image).copy()\n",
    "        image_array[:,159:162] = [255, 0, 0]                \n",
    "        steering_angle = float(model.predict(image_array[None, :, :, :], batch_size=1))\n",
    "\n",
    "        throttle = controller.update(float(speed))\n",
    "\n",
    "        print(steering_angle, throttle)\n",
    "        send_control(steering_angle, throttle)\n",
    "\n",
    "        # save frame\n",
    "        if args.image_folder != '':\n",
    "            timestamp = datetime.utcnow().strftime('%Y_%m_%d_%H_%M_%S_%f')[:-3]\n",
    "            image_filename = os.path.join(args.image_folder, timestamp)\n",
    "            image.save('{}.jpg'.format(image_filename))\n",
    "    else:\n",
    "        # NOTE: DON'T EDIT THIS.\n",
    "        sio.emit('manual', data={}, skip_sid=True)\n",
    "\n",
    "\n",
    "@sio.on('connect')\n",
    "def connect(sid, environ):\n",
    "    print(\"connect \", sid)\n",
    "    send_control(0, 0)\n",
    "\n",
    "\n",
    "def send_control(steering_angle, throttle):\n",
    "    sio.emit(\n",
    "        \"steer\",\n",
    "        data={\n",
    "            'steering_angle': steering_angle.__str__(),\n",
    "            'throttle': throttle.__str__()\n",
    "        },\n",
    "        skip_sid=True)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Remote Driving')\n",
    "    parser.add_argument(\n",
    "        'model',\n",
    "        type=str,\n",
    "        help='Path to model h5 file. Model should be on the same path.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        'image_folder',\n",
    "        type=str,\n",
    "        nargs='?',\n",
    "        default='',\n",
    "        help='Path to image folder. This is where the images from the run will be saved.'\n",
    "    )\n",
    "    #parser.parse_args()    \n",
    "    #parser.parse_args(['--sum', '7', '-1', '42'])    \n",
    "    args = parser.parse_args(['model.h5'])\n",
    "\n",
    "    # check that model Keras version is same as local Keras version\n",
    "    f = h5py.File(args.model, mode='r')\n",
    "    model_version = f.attrs.get('keras_version')\n",
    "    keras_version = str(keras_version).encode('utf8')\n",
    "\n",
    "    if model_version != keras_version:\n",
    "        print('You are using Keras version ', keras_version,\n",
    "              ', but the model was built using ', model_version)\n",
    "\n",
    "    #mine\n",
    "    print(args.model)\n",
    "    model = load_model(args.model)\n",
    "\n",
    "    if args.image_folder != '':\n",
    "        print(\"Creating image folder at {}\".format(args.image_folder))\n",
    "        if not os.path.exists(args.image_folder):\n",
    "            os.makedirs(args.image_folder)\n",
    "        else:\n",
    "            shutil.rmtree(args.image_folder)\n",
    "            os.makedirs(args.image_folder)\n",
    "        print(\"RECORDING THIS RUN ...\")\n",
    "    else:\n",
    "        print(\"NOT RECORDING THIS RUN ...\")\n",
    "\n",
    "    # wrap Flask application with engineio's middleware\n",
    "    app = socketio.Middleware(sio, app)\n",
    "\n",
    "    # deploy as an eventlet WSGI server\n",
    "    eventlet.wsgi.server(eventlet.listen(('', 4567)), app)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-milwaukee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-strike",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socketio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bcl] *",
   "language": "python",
   "name": "conda-env-bcl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
