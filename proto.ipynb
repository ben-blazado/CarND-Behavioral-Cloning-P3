{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "mighty-noise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:958: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:958: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:680: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:680: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:945: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:945: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2378: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2378: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:159: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:159: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:164: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:164: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:173: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:173: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:182: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:182: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:189: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:189: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387/387 [==============================] - 104s 270ms/step - loss: 0.0911 - val_loss: 0.0526\n",
      "Epoch 2/30\n",
      "387/387 [==============================] - 96s 248ms/step - loss: 0.0405 - val_loss: 0.0569\n",
      "Epoch 3/30\n",
      "387/387 [==============================] - 97s 251ms/step - loss: 0.0323 - val_loss: 0.0598\n",
      "Epoch 4/30\n",
      "387/387 [==============================] - 95s 245ms/step - loss: 0.0273 - val_loss: 0.0452\n",
      "Epoch 5/30\n",
      "387/387 [==============================] - 97s 250ms/step - loss: 0.0292 - val_loss: 0.0449\n",
      "Epoch 6/30\n",
      "387/387 [==============================] - 92s 238ms/step - loss: 0.0255 - val_loss: 0.0417\n",
      "Epoch 7/30\n",
      "387/387 [==============================] - 94s 243ms/step - loss: 0.0270 - val_loss: 0.0435\n",
      "Epoch 8/30\n",
      "387/387 [==============================] - 94s 242ms/step - loss: 0.0286 - val_loss: 0.0479\n",
      "Epoch 9/30\n",
      "387/387 [==============================] - 91s 234ms/step - loss: 0.0256 - val_loss: 0.0513\n",
      "Epoch 1/30\n",
      "387/387 [==============================] - 100s 259ms/step - loss: 0.1033 - val_loss: 0.0648\n",
      "Epoch 2/30\n",
      " 17/387 [>.............................] - ETA: 1:15 - loss: 0.0731"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-e45a9202a61d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\git\\CarND-Behavioral-Cloning-P3\\model.py\u001b[0m in \u001b[0;36mcreate_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m    152\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdriving_log_seq_validation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m                         callbacks=[early_stopper])\n\u001b[0m\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1225\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1227\u001b[1;33m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1229\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2145\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2146\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2147\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2149\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1837\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1838\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1839\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1840\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2357\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bcl\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bcl\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1180\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bcl\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bcl\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bcl\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bcl\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from model import create_model\n",
    "\n",
    "create_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-oliver",
   "metadata": {},
   "source": [
    "# Data Generator\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "respiratory-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil, floor\n",
    "from random import shuffle\n",
    "from keras.utils import Sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "theta       = 0.35\n",
    "corrections = {'center': 0, 'left': theta, 'right': -theta}\n",
    "\n",
    "dx          = 50\n",
    "shift_val   = {'left': dx, 'right': -dx}\n",
    "\n",
    "\n",
    "def shift(img, dx):\n",
    "        \n",
    "    shifted_img = img.copy()\n",
    "    if dx > 0:\n",
    "        # shift right\n",
    "        shifted_img[:,dx:] = img[:,:-dx]\n",
    "    else:\n",
    "        # shift left\n",
    "        shifted_img[:,:dx] = img[:,-dx:]\n",
    "\n",
    "    return shifted_img\n",
    "\n",
    "\n",
    "# ref: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "class DrivingLogSequence(Sequence):\n",
    "    \n",
    "    def __init__ (self, driving_log, batch_size=32):\n",
    "        \n",
    "        self.driving_log = driving_log\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def __len__(self):\n",
    "        return floor(len(self.driving_log)/self.batch_size)\n",
    "    \n",
    "    def add_data(self, img, steering):\n",
    "        self.images.append(img)\n",
    "        self.steerings.append([steering])\n",
    "        return\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        batch_driving_log = self.driving_log[index:index+self.batch_size]\n",
    "        self.images = []\n",
    "        self.steerings = []\n",
    "        \n",
    "        for line in batch_driving_log:\n",
    "            \n",
    "            center_steering = float(line['steering'])\n",
    "            \n",
    "            for camera in corrections:\n",
    "                \n",
    "                filename = line[camera]\n",
    "                \n",
    "                img = plt.imread(filename).copy()\n",
    "                steering = center_steering + corrections[camera]\n",
    "                \n",
    "                self.add_data(img,           steering)\n",
    "                self.add_data(np.fliplr(img), -steering)\n",
    "                \n",
    "                if camera != 'center':\n",
    "                    steering  = steering + corrections[camera]\n",
    "                    shift_img = shift(img, shift_val[camera])\n",
    "                    \n",
    "                    self.add_data(shift_img,            steering)\n",
    "                    self.add_data(np.fliplr(shift_img), -steering)\n",
    "            \n",
    "        X = np.array(self.images)\n",
    "        y = np.array(self.steerings)\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        shuffle(self.driving_log)\n",
    "        return\n",
    "    \n",
    "\n",
    "def create_generators(test_size=0.2, shuffle=True):\n",
    "    \n",
    "    drv_log_folders = ['data_easy_route','data_hard_route']\n",
    "    drv_log_filename = 'driving_log.csv'\n",
    "\n",
    "    drv_log = []\n",
    "    for drv_log_folder in drv_log_folders:\n",
    "        with open(drv_log_folder + '/' + drv_log_filename) as driving_log_file:\n",
    "            driving_log_reader = csv.DictReader(driving_log_file)\n",
    "            for line in driving_log_reader:\n",
    "                for camera in corrections:\n",
    "                    line[camera] = drv_log_folder + \"/\" + line[camera].strip()\n",
    "                drv_log.append(line)\n",
    "                \n",
    "    training, validation = train_test_split(drv_log, test_size=0.2, shuffle=True)                                       \n",
    "                       \n",
    "    return DrivingLogSequence(training), DrivingLogSequence(validation)        \n",
    "                       \n",
    "                       \n",
    "driving_log_seq_training, driving_log_seq_validation = create_generators()\n",
    "                       \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-atlas",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def OLD__getitem__OLD(self, index):\n",
    "        \n",
    "        batch_driving_log = self.driving_log[index:index+self.batch_size]\n",
    "        images = []\n",
    "        steerings = []\n",
    "        \n",
    "        for line in batch_driving_log:\n",
    "            \n",
    "            center_steering = float(line['steering'])\n",
    "            \n",
    "            for cam_pos, theta in cameras:\n",
    "                \n",
    "                filename = line[cam_pos].strip()\n",
    "                \n",
    "                img = plt.imread(filename).copy()\n",
    "                images.append(img)\n",
    "                steering = center_steering + theta\n",
    "                steerings.append([steering])\n",
    "                \n",
    "                # augment with flipped image\n",
    "                flip_img = np.fliplr(img)\n",
    "                images.append(flip_img)\n",
    "                steerings.append([-steering])\n",
    "                \n",
    "                if cam_pos == 'left':\n",
    "                    # augment with right shift\n",
    "                    rightshift_img = shift(img, 50)\n",
    "                    images.append(rightshift_img)\n",
    "                    steerings.append([steering + theta])\n",
    "\n",
    "                    # augment with flipped right shift image\n",
    "                    flip_img = np.fliplr(rightshift_img)\n",
    "                    images.append(flip_img)\n",
    "                    steerings.append([-(steering + theta)])\n",
    "                    \n",
    "                elif cam_pos == 'right':\n",
    "                    # augment with left shift\n",
    "                    leftshift_img = shift(img, -50)\n",
    "                    images.append(leftshift_img)\n",
    "                    steerings.append([steering - theta])\n",
    "\n",
    "                    # augment with flipped left shift image\n",
    "                    flip_img = np.fliplr(leftshift_img)\n",
    "                    images.append(flip_img)\n",
    "                    steerings.append([-(steering - theta)])\n",
    "            \n",
    "        X = np.array(images)\n",
    "        y = np.array(steerings)\n",
    "        \n",
    "        return X, y\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-essay",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "driving_log_filename = \n",
    "\n",
    "driving_log = []\n",
    "\n",
    "with open(driving_log_filename) as driving_log_file:\n",
    "    driving_log_reader = csv.DictReader(driving_log_file)\n",
    "    for line in driving_log_reader:\n",
    "        driving_log.append(line)\n",
    "        \n",
    "driving_log_filename = 'data/driving_log.csv'\n",
    "\n",
    "with open(driving_log_filename) as driving_log_file:\n",
    "    driving_log_reader = csv.DictReader(driving_log_file)\n",
    "    for line in driving_log_reader:\n",
    "        driving_log.append(line)\n",
    "        \n",
    "        \n",
    "driving_log_training, driving_log_validation = train_test_split(driving_log, test_size=0.2, shuffle=True)\n",
    "driving_log_seq_training   = DrivingLogSequence(driving_log_training)\n",
    "driving_log_seq_validation = DrivingLogSequence(driving_log_validation)        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-penetration",
   "metadata": {},
   "source": [
    "### Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-argentina",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_name = \"sample_data/IMG/left_2016_12_01_13_30_48_287.jpg\"\n",
    "img=plt.imread(\"sample_data/IMG/left_2016_12_01_13_30_48_287.jpg\")\n",
    "shift_img = shift(img, -50)\n",
    "plt.imshow(shift_img)\n",
    "\n",
    "#img_copy = np.copy(img)\n",
    "#img_copy[:,159:162] = [255, 0, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-strike",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def shift(img, dx):\n",
    "\n",
    "    shifted_img = np.zeros_like(img)\n",
    "    if dx > 0:\n",
    "        # shift right\n",
    "        shifted_img[:,dx:] = img[:,:-dx]\n",
    "    else:\n",
    "        # shift left\n",
    "        shifted_img[:,:dx] = img[:,-dx:]\n",
    "\n",
    "    return shifted_img\n",
    "\n",
    "shift_img = shift(img, -75)\n",
    "plt.imshow(shift_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-glenn",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-quest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "driving_log_filename = 'sample_data/driving_log.csv'\n",
    "\n",
    "center_images = []\n",
    "steerings = []\n",
    "with open(driving_log_filename) as driving_log:\n",
    "    driving_log_reader = csv.DictReader(driving_log)\n",
    "    for row in driving_log_reader:\n",
    "        center_images.append(plt.imread('sample_data/' + row['center']))\n",
    "        steerings.append([float(row['steering'])])\n",
    "                            \n",
    "X_train = np.array(center_images)\n",
    "y_train = np.array(steerings)\n",
    "    \n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-civilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding left and right cameras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "driving_log_filename = 'sample_data/driving_log.csv'\n",
    "\n",
    "images = []\n",
    "steerings = []\n",
    "theta = 0.35\n",
    "with open(driving_log_filename) as driving_log:\n",
    "    driving_log_reader = csv.DictReader(driving_log)\n",
    "    for row in driving_log_reader:\n",
    "        \n",
    "        steering = float(row['steering'])\n",
    "        \n",
    "        images.append(plt.imread('sample_data/' + row['center'].strip()))\n",
    "        steerings.append([steering])\n",
    "        \n",
    "        images.append(plt.imread('sample_data/' + row['left'].strip()))\n",
    "        steerings.append([steering + theta])\n",
    "        \n",
    "        images.append(plt.imread('sample_data/' + row['right'].strip()))\n",
    "        steerings.append([steering - theta])\n",
    "        \n",
    "                            \n",
    "X_train = np.array(images)\n",
    "y_train = np.array(steerings)\n",
    "    \n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-gospel",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "'''\n",
    "def generator(lines, batch_size=32):\n",
    "\n",
    "    driving_log_filename = 'sample_data/driving_log.csv'\n",
    "\n",
    "    images = []\n",
    "    steerings = []\n",
    "    theta = 0.35\n",
    "    \n",
    "    with open(driving_log_filename) as driving_log:\n",
    "        num_lines = 0\n",
    "        for line in driving_log:\n",
    "            num_lines += 1\n",
    "\n",
    "    with open(driving_log_filename) as driving_log:\n",
    "\n",
    "        driving_log_reader = csv.DictReader(driving_log)\n",
    "        \n",
    "        \n",
    "        print (len(driving_log_reader))\n",
    "\n",
    "    for row in driving_log_reader:\n",
    "\n",
    "        steering = float(row['steering'])\n",
    "\n",
    "        images.append(plt.imread('sample_data/' + row['center'].strip()))\n",
    "        steerings.append([steering])\n",
    "\n",
    "        images.append(plt.imread('sample_data/' + row['left'].strip()))\n",
    "        steerings.append([steering + theta])\n",
    "\n",
    "        images.append(plt.imread('sample_data/' + row['right'].strip()))\n",
    "        steerings.append([steering - theta])\n",
    "    '''\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-effects",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[46]\n",
    "\n",
    "for y in y_train:\n",
    "    if y > 0:\n",
    "        print(y)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-haiti",
   "metadata": {},
   "source": [
    "# Network Structure\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "innocent-casting",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:64: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:64: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:497: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:497: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3636: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3636: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3019: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3019: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1264: calling reduce_prod_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1264: calling reduce_prod_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nmodel = Sequential()\\nmodel.add(Lambda(normalize, input_shape=(160,320,3)))\\nmodel.add(Cropping2D(cropping=[(50, 20), (0, 0)]))\\nmodel.add(Conv2D(filters=24, kernel_size=5, strides=2, activation='relu'))\\nmodel.add(Dropout(0.2))\\nmodel.add(BatchNormalization())\\nmodel.add(Conv2D(filters=36, kernel_size=5, strides=2, activation='relu'))\\nmodel.add(Dropout(0.2))\\nmodel.add(BatchNormalization())\\nmodel.add(Conv2D(filters=48, kernel_size=5, strides=2, activation='relu'))\\nmodel.add(BatchNormalization())\\nmodel.add(Conv2D(filters=64, kernel_size=3, strides=1, activation='relu'))\\nmodel.add(BatchNormalization())\\nmodel.add(Conv2D(filters=64, kernel_size=3, strides=1, activation='relu'))\\nmodel.add(BatchNormalization())\\nmodel.add(Flatten())\\nmodel.add(Dense(100, activation='relu'))\\nmodel.add(Dropout(0.5))\\nmodel.add(BatchNormalization())\\nmodel.add(Dense(50, activation='relu'))\\nmodel.add(Dropout(0.5))\\nmodel.add(BatchNormalization())\\nmodel.add(Dense(10, activation='relu'))\\nmodel.add(Dense(1))\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Lambda, BatchNormalization, Flatten, Dense \n",
    "from keras.layers import Conv2D, Cropping2D, Dropout, MaxPooling2D\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def normalize(rgb):\n",
    "    '''\n",
    "    normalize rgb between [-1, 1]\n",
    "    '''\n",
    "    \n",
    "    return (rgb-128.0) / 128.0\n",
    "\n",
    "\n",
    "# ---loss: 0.0175\n",
    "model = Sequential()\n",
    "model.add(Lambda(normalize, input_shape=(160,320,3)))\n",
    "model.add(Cropping2D(cropping=[(50, 20), (0, 0)]))\n",
    "model.add(Conv2D(filters=24, kernel_size=5, strides=2, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Conv2D(filters=36, kernel_size=5, strides=2, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Conv2D(filters=48, kernel_size=5, strides=2, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(filters=64, kernel_size=3, strides=1, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(filters=64, kernel_size=3, strides=1, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "'''\n",
    "model = Sequential()\n",
    "model.add(Lambda(normalize, input_shape=(160,320,3)))\n",
    "model.add(Cropping2D(cropping=[(50, 20), (0, 0)]))\n",
    "model.add(Conv2D(filters=24, kernel_size=5, strides=2, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=36, kernel_size=5, strides=2, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=48, kernel_size=5, strides=2, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=64, kernel_size=3, strides=1, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=64, kernel_size=3, strides=1, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "'''\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "amber-cattle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_10 (Batc (None, 160, 320, 3)       12        \n",
      "_________________________________________________________________\n",
      "cropping2d_2 (Cropping2D)    (None, 90, 320, 3)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 43, 158, 24)       1824      \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 43, 158, 24)       96        \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 20, 77, 36)        21636     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 20, 77, 36)        144       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 8, 37, 48)         43248     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 8, 37, 48)         192       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 6, 35, 64)         27712     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 6, 35, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 4, 33, 64)         36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 4, 33, 64)         256       \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8448)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               844900    \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 983,415\n",
      "Trainable params: 982,617\n",
      "Non-trainable params: 798\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "comparable-public",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='MSE', optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "close-thong",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:958: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:958: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:680: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:680: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:945: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:945: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2378: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2378: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:159: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:159: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:164: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:164: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:173: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:173: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:182: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:182: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:189: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\bcl\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:189: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387/387 [==============================] - 142s 367ms/step - loss: 0.0621 - val_loss: 0.1012\n",
      "Epoch 2/30\n",
      "387/387 [==============================] - 130s 336ms/step - loss: 0.0185 - val_loss: 0.0667\n",
      "Epoch 3/30\n",
      "387/387 [==============================] - 131s 339ms/step - loss: 0.0104 - val_loss: 0.0569\n",
      "Epoch 4/30\n",
      "387/387 [==============================] - 131s 339ms/step - loss: 0.0088 - val_loss: 0.0623\n",
      "Epoch 5/30\n",
      "387/387 [==============================] - 130s 336ms/step - loss: 0.0080 - val_loss: 0.0479\n",
      "Epoch 6/30\n",
      "387/387 [==============================] - 130s 337ms/step - loss: 0.0065 - val_loss: 0.0447\n",
      "Epoch 7/30\n",
      "387/387 [==============================] - 133s 344ms/step - loss: 0.0079 - val_loss: 0.0650\n",
      "Epoch 8/30\n",
      "387/387 [==============================] - 132s 341ms/step - loss: 0.0060 - val_loss: 0.0350\n",
      "Epoch 9/30\n",
      "387/387 [==============================] - 128s 332ms/step - loss: 0.0066 - val_loss: 0.0370\n",
      "Epoch 10/30\n",
      "387/387 [==============================] - 132s 340ms/step - loss: 0.0050 - val_loss: 0.0457\n",
      "Epoch 11/30\n",
      "387/387 [==============================] - 131s 339ms/step - loss: 0.0047 - val_loss: 0.0362\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# https://keras.io/api/callbacks/early_stopping/\n",
    "early_stopper = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "model.fit_generator(driving_log_seq_training, \n",
    "                    validation_data=driving_log_seq_validation, \n",
    "                    epochs=30,\n",
    "                    callbacks=[early_stopper])\n",
    "\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "unlimited-worker",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-central",
   "metadata": {},
   "source": [
    "### Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-island",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, batch_size=512, initial_epoch=0, epochs=4, shuffle=True, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-bicycle",
   "metadata": {},
   "source": [
    "# drive.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-balance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import base64\n",
    "from datetime import datetime\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import socketio\n",
    "import eventlet\n",
    "import eventlet.wsgi\n",
    "from PIL import Image\n",
    "from flask import Flask\n",
    "from io import BytesIO\n",
    "\n",
    "from keras.models import load_model\n",
    "import h5py\n",
    "from keras import __version__ as keras_version\n",
    "\n",
    "sio = socketio.Server()\n",
    "app = Flask(__name__)\n",
    "model = None\n",
    "prev_image_array = None\n",
    "\n",
    "\n",
    "class SimplePIController:\n",
    "    def __init__(self, Kp, Ki):\n",
    "        self.Kp = Kp\n",
    "        self.Ki = Ki\n",
    "        self.set_point = 0.\n",
    "        self.error = 0.\n",
    "        self.integral = 0.\n",
    "\n",
    "    def set_desired(self, desired):\n",
    "        self.set_point = desired\n",
    "\n",
    "    def update(self, measurement):\n",
    "        # proportional error\n",
    "        self.error = self.set_point - measurement\n",
    "\n",
    "        # integral error\n",
    "        self.integral += self.error\n",
    "\n",
    "        return self.Kp * self.error + self.Ki * self.integral\n",
    "\n",
    "\n",
    "controller = SimplePIController(0.1, 0.002)\n",
    "set_speed = 9\n",
    "controller.set_desired(set_speed)\n",
    "\n",
    "\n",
    "@sio.on('telemetry')\n",
    "def telemetry(sid, data):\n",
    "    if data:\n",
    "        # The current steering angle of the car\n",
    "        steering_angle = data[\"steering_angle\"]\n",
    "        # The current throttle of the car\n",
    "        throttle = data[\"throttle\"]\n",
    "        # The current speed of the car\n",
    "        speed = data[\"speed\"]\n",
    "        # The current image from the center camera of the car\n",
    "        imgString = data[\"image\"]\n",
    "        image = Image.open(BytesIO(base64.b64decode(imgString)))\n",
    "        image_array = np.asarray(image).copy()\n",
    "        image_array[:,159:162] = [255, 0, 0]                \n",
    "        steering_angle = float(model.predict(image_array[None, :, :, :], batch_size=1))\n",
    "\n",
    "        throttle = controller.update(float(speed))\n",
    "\n",
    "        print(steering_angle, throttle)\n",
    "        send_control(steering_angle, throttle)\n",
    "\n",
    "        # save frame\n",
    "        if args.image_folder != '':\n",
    "            timestamp = datetime.utcnow().strftime('%Y_%m_%d_%H_%M_%S_%f')[:-3]\n",
    "            image_filename = os.path.join(args.image_folder, timestamp)\n",
    "            image.save('{}.jpg'.format(image_filename))\n",
    "    else:\n",
    "        # NOTE: DON'T EDIT THIS.\n",
    "        sio.emit('manual', data={}, skip_sid=True)\n",
    "\n",
    "\n",
    "@sio.on('connect')\n",
    "def connect(sid, environ):\n",
    "    print(\"connect \", sid)\n",
    "    send_control(0, 0)\n",
    "\n",
    "\n",
    "def send_control(steering_angle, throttle):\n",
    "    sio.emit(\n",
    "        \"steer\",\n",
    "        data={\n",
    "            'steering_angle': steering_angle.__str__(),\n",
    "            'throttle': throttle.__str__()\n",
    "        },\n",
    "        skip_sid=True)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Remote Driving')\n",
    "    parser.add_argument(\n",
    "        'model',\n",
    "        type=str,\n",
    "        help='Path to model h5 file. Model should be on the same path.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        'image_folder',\n",
    "        type=str,\n",
    "        nargs='?',\n",
    "        default='',\n",
    "        help='Path to image folder. This is where the images from the run will be saved.'\n",
    "    )\n",
    "    #parser.parse_args()    \n",
    "    #parser.parse_args(['--sum', '7', '-1', '42'])    \n",
    "    args = parser.parse_args(['model.h5'])\n",
    "\n",
    "    # check that model Keras version is same as local Keras version\n",
    "    f = h5py.File(args.model, mode='r')\n",
    "    model_version = f.attrs.get('keras_version')\n",
    "    keras_version = str(keras_version).encode('utf8')\n",
    "\n",
    "    if model_version != keras_version:\n",
    "        print('You are using Keras version ', keras_version,\n",
    "              ', but the model was built using ', model_version)\n",
    "\n",
    "    #mine\n",
    "    print(args.model)\n",
    "    model = load_model(args.model)\n",
    "\n",
    "    if args.image_folder != '':\n",
    "        print(\"Creating image folder at {}\".format(args.image_folder))\n",
    "        if not os.path.exists(args.image_folder):\n",
    "            os.makedirs(args.image_folder)\n",
    "        else:\n",
    "            shutil.rmtree(args.image_folder)\n",
    "            os.makedirs(args.image_folder)\n",
    "        print(\"RECORDING THIS RUN ...\")\n",
    "    else:\n",
    "        print(\"NOT RECORDING THIS RUN ...\")\n",
    "\n",
    "    # wrap Flask application with engineio's middleware\n",
    "    app = socketio.Middleware(sio, app)\n",
    "\n",
    "    # deploy as an eventlet WSGI server\n",
    "    eventlet.wsgi.server(eventlet.listen(('', 4567)), app)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-milwaukee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-strike",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socketio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bcl] *",
   "language": "python",
   "name": "conda-env-bcl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
